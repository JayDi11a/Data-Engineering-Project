[2024-06-29T18:27:15.654+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-06-29T18:26:38.712167+00:00 [queued]>
[2024-06-29T18:27:15.658+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-06-29T18:26:38.712167+00:00 [queued]>
[2024-06-29T18:27:15.658+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2024-06-29T18:27:15.664+0000] {taskinstance.py:1382} INFO - Executing <Task(DockerOperator): pyspark_consumer> on 2024-06-29 18:26:38.712167+00:00
[2024-06-29T18:27:15.667+0000] {standard_task_runner.py:57} INFO - Started process 909 to run task
[2024-06-29T18:27:15.669+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'kafka_spark_dag', 'pyspark_consumer', 'manual__2024-06-29T18:26:38.712167+00:00', '--job-id', '8', '--raw', '--subdir', 'DAGS_FOLDER/dag_kafka_spark.py', '--cfg-path', '/tmp/tmpd7nfpll3']
[2024-06-29T18:27:15.671+0000] {standard_task_runner.py:85} INFO - Job 8: Subtask pyspark_consumer
[2024-06-29T18:27:15.694+0000] {task_command.py:416} INFO - Running <TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-06-29T18:26:38.712167+00:00 [running]> on host 071a9383ecd5
[2024-06-29T18:27:15.730+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='kafka_spark_dag' AIRFLOW_CTX_TASK_ID='pyspark_consumer' AIRFLOW_CTX_EXECUTION_DATE='2024-06-29T18:26:38.712167+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-29T18:26:38.712167+00:00'
[2024-06-29T18:27:15.749+0000] {docker.py:343} INFO - Starting docker container from image current-headlines/spark:latest
[2024-06-29T18:27:15.914+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m18:27:15.91 [0m[38;5;2mINFO [0m ==>
[2024-06-29T18:27:15.915+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m18:27:15.91 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-06-29T18:27:15.916+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m18:27:15.91 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-06-29T18:27:15.917+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m18:27:15.91 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-06-29T18:27:15.917+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m18:27:15.91 [0m[38;5;2mINFO [0m ==>
[2024-06-29T18:27:15.921+0000] {docker.py:413} INFO - 
[2024-06-29T18:27:16.915+0000] {docker.py:413} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-06-29T18:27:16.959+0000] {docker.py:413} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-06-29T18:27:16.962+0000] {docker.py:413} INFO - org.postgresql#postgresql added as a dependency
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-06-29T18:27:16.962+0000] {docker.py:413} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-62f42f26-1d1c-4743-a9c9-923e96bfcd72;1.0
	confs: [default]
[2024-06-29T18:27:17.393+0000] {docker.py:413} INFO - found org.postgresql#postgresql;42.5.4 in central
[2024-06-29T18:27:17.441+0000] {docker.py:413} INFO - found org.checkerframework#checker-qual;3.5.0 in central
[2024-06-29T18:27:18.110+0000] {docker.py:413} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2024-06-29T18:27:18.289+0000] {docker.py:413} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2024-06-29T18:27:18.340+0000] {docker.py:413} INFO - found org.apache.kafka#kafka-clients;3.4.1 in central
[2024-06-29T18:27:18.392+0000] {docker.py:413} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-06-29T18:27:18.451+0000] {docker.py:413} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-06-29T18:27:18.730+0000] {docker.py:413} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2024-06-29T18:27:19.397+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-06-29T18:27:19.468+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-06-29T18:27:20.014+0000] {docker.py:413} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-06-29T18:27:20.063+0000] {docker.py:413} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-06-29T18:27:20.838+0000] {docker.py:413} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-06-29T18:27:20.864+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.4/postgresql-42.5.4.jar ...
[2024-06-29T18:27:21.405+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.5.4!postgresql.jar (564ms)
[2024-06-29T18:27:21.432+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
[2024-06-29T18:27:21.641+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (235ms)
[2024-06-29T18:27:21.671+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...
[2024-06-29T18:27:21.772+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (130ms)
[2024-06-29T18:27:21.796+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
[2024-06-29T18:27:21.838+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (65ms)
[2024-06-29T18:27:21.863+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2024-06-29T18:27:23.877+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (2038ms)
[2024-06-29T18:27:23.896+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-06-29T18:27:23.932+0000] {docker.py:413} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (54ms)
[2024-06-29T18:27:23.953+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-06-29T18:27:24.035+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (101ms)
[2024-06-29T18:27:24.059+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2024-06-29T18:27:32.815+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (8777ms)
[2024-06-29T18:27:32.842+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-06-29T18:27:33.030+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (214ms)
[2024-06-29T18:27:33.053+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2024-06-29T18:27:33.642+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (610ms)
[2024-06-29T18:27:33.665+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2024-06-29T18:27:33.698+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (55ms)
[2024-06-29T18:27:33.736+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2024-06-29T18:27:38.196+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (4494ms)
[2024-06-29T18:27:38.218+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-06-29T18:27:38.266+0000] {docker.py:413} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (63ms)
:: resolution report :: resolve 3880ms :: artifacts dl 17418ms
	:: modules in use:
	com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
	org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
	org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
	org.checkerframework#checker-qual;3.5.0 from central in [default]
	org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.5.4 from central in [default]
	org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
	---------------------------------------------------------------------
	|                  |            modules            ||   artifacts   |
	|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
	|      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2024-06-29T18:27:38.268+0000] {docker.py:413} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-62f42f26-1d1c-4743-a9c9-923e96bfcd72
	confs: [default]
[2024-06-29T18:27:38.300+0000] {docker.py:413} INFO - 13 artifacts copied, 0 already retrieved (58001kB/32ms)
[2024-06-29T18:27:38.403+0000] {docker.py:413} INFO - 24/06/29 18:27:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-06-29T18:27:38.997+0000] {docker.py:413} INFO - 24/06/29 18:27:38 INFO SparkContext: Running Spark version 3.5.0
[2024-06-29T18:27:38.998+0000] {docker.py:413} INFO - 24/06/29 18:27:38 INFO SparkContext: OS info Linux, 6.6.12-linuxkit, aarch64
[2024-06-29T18:27:38.998+0000] {docker.py:413} INFO - 24/06/29 18:27:38 INFO SparkContext: Java version 17.0.10
[2024-06-29T18:27:39.009+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO ResourceUtils: ==============================================================
[2024-06-29T18:27:39.009+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-06-29T18:27:39.009+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO ResourceUtils: ==============================================================
[2024-06-29T18:27:39.010+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Submitted application: PostgreSQL Connection with PySpark
[2024-06-29T18:27:39.020+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-06-29T18:27:39.025+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO ResourceProfile: Limiting resource is cpu
[2024-06-29T18:27:39.025+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-06-29T18:27:39.052+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SecurityManager: Changing view acls to: spark
[2024-06-29T18:27:39.052+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SecurityManager: Changing modify acls to: spark
[2024-06-29T18:27:39.053+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SecurityManager: Changing view acls groups to:
[2024-06-29T18:27:39.053+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SecurityManager: Changing modify acls groups to:
[2024-06-29T18:27:39.053+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-06-29T18:27:39.168+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Successfully started service 'sparkDriver' on port 44305.
[2024-06-29T18:27:39.182+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkEnv: Registering MapOutputTracker
[2024-06-29T18:27:39.201+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkEnv: Registering BlockManagerMaster
[2024-06-29T18:27:39.212+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-06-29T18:27:39.212+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-06-29T18:27:39.214+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-06-29T18:27:39.228+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d5e1d075-70c5-477f-97fe-d4b32da59104
[2024-06-29T18:27:39.236+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-06-29T18:27:39.244+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-06-29T18:27:39.318+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-06-29T18:27:39.353+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-06-29T18:27:39.374+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar at spark://localhost:44305/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.375+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://localhost:44305/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.375+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar at spark://localhost:44305/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.375+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://localhost:44305/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.376+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://localhost:44305/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719685658993
[2024-06-29T18:27:39.376+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://localhost:44305/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.376+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://localhost:44305/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719685658993
[2024-06-29T18:27:39.377+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://localhost:44305/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.377+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://localhost:44305/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.377+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://localhost:44305/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719685658993
[2024-06-29T18:27:39.377+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://localhost:44305/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719685658993
[2024-06-29T18:27:39.378+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://localhost:44305/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.378+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://localhost:44305/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719685658993
[2024-06-29T18:27:39.378+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.378+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.postgresql_postgresql-42.5.4.jar
[2024-06-29T18:27:39.382+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.382+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T18:27:39.384+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.384+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.checkerframework_checker-qual-3.5.0.jar
[2024-06-29T18:27:39.386+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.387+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T18:27:39.388+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719685658993
[2024-06-29T18:27:39.388+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-06-29T18:27:39.391+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719685658993
24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-29T18:27:39.393+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719685658993
24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-29T18:27:39.395+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.395+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-06-29T18:27:39.407+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719685658993
24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.lz4_lz4-java-1.8.0.jar
[2024-06-29T18:27:39.409+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719685658993
[2024-06-29T18:27:39.409+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-06-29T18:27:39.411+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719685658993
[2024-06-29T18:27:39.411+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.slf4j_slf4j-api-2.0.7.jar
[2024-06-29T18:27:39.413+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.413+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-06-29T18:27:39.421+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719685658993
[2024-06-29T18:27:39.422+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/commons-logging_commons-logging-1.1.3.jar
[2024-06-29T18:27:39.467+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Starting executor ID driver on host localhost
[2024-06-29T18:27:39.467+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: OS info Linux, 6.6.12-linuxkit, aarch64
[2024-06-29T18:27:39.467+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Java version 17.0.10
[2024-06-29T18:27:39.471+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-06-29T18:27:39.471+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1fa02dda for default.
[2024-06-29T18:27:39.477+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.485+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T18:27:39.487+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.499+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-06-29T18:27:39.501+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719685658993
[2024-06-29T18:27:39.501+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-29T18:27:39.503+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.503+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.postgresql_postgresql-42.5.4.jar
[2024-06-29T18:27:39.508+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719685658993
[2024-06-29T18:27:39.509+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.slf4j_slf4j-api-2.0.7.jar
[2024-06-29T18:27:39.509+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719685658993
[2024-06-29T18:27:39.509+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-06-29T18:27:39.510+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.511+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-29T18:27:39.513+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.513+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.lz4_lz4-java-1.8.0.jar
[2024-06-29T18:27:39.515+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.515+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T18:27:39.517+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.523+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-06-29T18:27:39.525+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719685658993
[2024-06-29T18:27:39.525+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/commons-logging_commons-logging-1.1.3.jar
[2024-06-29T18:27:39.527+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.527+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.checkerframework_checker-qual-3.5.0.jar
[2024-06-29T18:27:39.529+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719685658993
[2024-06-29T18:27:39.530+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-06-29T18:27:39.532+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719685658993
[2024-06-29T18:27:39.560+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:44305 after 18 ms (0 ms spent in bootstraps)
[2024-06-29T18:27:39.566+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp6542674278260773149.tmp
[2024-06-29T18:27:39.577+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp6542674278260773149.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/commons-logging_commons-logging-1.1.3.jar
[2024-06-29T18:27:39.580+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/commons-logging_commons-logging-1.1.3.jar to class loader default
[2024-06-29T18:27:39.580+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719685658993
[2024-06-29T18:27:39.581+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp13528501177012368342.tmp
[2024-06-29T18:27:39.581+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp13528501177012368342.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-29T18:27:39.583+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2024-06-29T18:27:39.584+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.585+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp12946232432192087149.tmp
[2024-06-29T18:27:39.588+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp12946232432192087149.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T18:27:39.590+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
[2024-06-29T18:27:39.593+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.594+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp11001422141816708580.tmp
[2024-06-29T18:27:39.595+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp11001422141816708580.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-29T18:27:39.597+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719685658993
[2024-06-29T18:27:39.597+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp17498431206534067441.tmp
[2024-06-29T18:27:39.598+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp17498431206534067441.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.slf4j_slf4j-api-2.0.7.jar
[2024-06-29T18:27:39.600+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.slf4j_slf4j-api-2.0.7.jar to class loader default
[2024-06-29T18:27:39.601+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.601+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp14410656328893843987.tmp
[2024-06-29T18:27:39.663+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp14410656328893843987.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-06-29T18:27:39.667+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[2024-06-29T18:27:39.667+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.668+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp17508183244638858420.tmp
[2024-06-29T18:27:39.668+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp17508183244638858420.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T18:27:39.670+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
[2024-06-29T18:27:39.671+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.672+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.postgresql_postgresql-42.5.4.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp16008257576347340345.tmp
[2024-06-29T18:27:39.674+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp16008257576347340345.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.postgresql_postgresql-42.5.4.jar
[2024-06-29T18:27:39.676+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.postgresql_postgresql-42.5.4.jar to class loader default
[2024-06-29T18:27:39.676+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719685658993
24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp377687659119328395.tmp
[2024-06-29T18:27:39.680+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp377687659119328395.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-06-29T18:27:39.682+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.683+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.checkerframework_checker-qual-3.5.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp845857278893729339.tmp
[2024-06-29T18:27:39.686+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp845857278893729339.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.checkerframework_checker-qual-3.5.0.jar
[2024-06-29T18:27:39.687+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.checkerframework_checker-qual-3.5.0.jar to class loader default
[2024-06-29T18:27:39.687+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719685658993
[2024-06-29T18:27:39.688+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp14541801868471036853.tmp
[2024-06-29T18:27:39.712+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp14541801868471036853.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-06-29T18:27:39.716+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
[2024-06-29T18:27:39.717+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719685658993
[2024-06-29T18:27:39.717+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp15569828430353607126.tmp
[2024-06-29T18:27:39.724+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp15569828430353607126.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-06-29T18:27:39.727+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
[2024-06-29T18:27:39.728+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Fetching spark://localhost:44305/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719685658993
[2024-06-29T18:27:39.728+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Fetching spark://localhost:44305/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp17239819405190134791.tmp
[2024-06-29T18:27:39.729+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/fetchFileTemp17239819405190134791.tmp has been previously copied to /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.lz4_lz4-java-1.8.0.jar
[2024-06-29T18:27:39.730+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Executor: Adding file:/tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/userFiles-6d698e91-ec52-4e87-bc5d-7c015baa109e/org.lz4_lz4-java-1.8.0.jar to class loader default
[2024-06-29T18:27:39.735+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44765.
[2024-06-29T18:27:39.735+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO NettyBlockTransferService: Server created on localhost:44765
[2024-06-29T18:27:39.736+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-06-29T18:27:39.739+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 44765, None)
[2024-06-29T18:27:39.741+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO BlockManagerMasterEndpoint: Registering block manager localhost:44765 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 44765, None)
[2024-06-29T18:27:39.742+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 44765, None)
[2024-06-29T18:27:39.743+0000] {docker.py:413} INFO - 24/06/29 18:27:39 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 44765, None)
[2024-06-29T18:27:39.998+0000] {docker.py:413} INFO - 2024-06-29 18:27:39,997:create_spark_session:INFO:Spark session created successfully
[2024-06-29T18:27:40.002+0000] {docker.py:413} INFO - 24/06/29 18:27:40 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-06-29T18:27:40.002+0000] {docker.py:413} INFO - 24/06/29 18:27:40 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2024-06-29T18:27:40.799+0000] {docker.py:413} INFO - 2024-06-29 18:27:40,798:create_initial_dataframe:INFO:Initial dataframe created successfully
[2024-06-29T18:27:41.133+0000] {docker.py:413} INFO - 2024-06-29 18:27:41,132:start_streaming:INFO:Start streaming ...
[2024-06-29T18:27:41.253+0000] {docker.py:413} INFO - 2024-06-29 18:27:41,251:run:INFO:Callback Server Starting
2024-06-29 18:27:41,251:run:INFO:Socket listening on ('127.0.0.1', 46311)
[2024-06-29T18:27:41.266+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-06-29T18:27:41.282+0000] {docker.py:413} INFO - 24/06/29 18:27:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-06-29T18:27:41.297+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0 resolved to file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0.
[2024-06-29T18:27:41.297+0000] {docker.py:413} INFO - 24/06/29 18:27:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-06-29T18:27:41.335+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/metadata using temp file file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/.metadata.827e4f33-7a0c-4b40-bbb2-d3b17149888f.tmp
[2024-06-29T18:27:41.395+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/.metadata.827e4f33-7a0c-4b40-bbb2-d3b17149888f.tmp to file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/metadata
[2024-06-29T18:27:41.420+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO MicroBatchExecution: Starting [id = 2e07d8e7-d7b8-4abe-9b70-b84cfffebb7f, runId = e244d99d-3e59-47e1-a09f-40b7cca665c8]. Use file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0 to store the query checkpoint.
[2024-06-29T18:27:41.427+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@6db44ccd] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1038ec4f]
[2024-06-29T18:27:41.451+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-29T18:27:41.452+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-29T18:27:41.453+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO MicroBatchExecution: Starting new streaming query.
[2024-06-29T18:27:41.453+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO MicroBatchExecution: Stream started from {}
[2024-06-29T18:27:41.625+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
[2024-06-29T18:27:41.667+0000] {docker.py:413} INFO - 24/06/29 18:27:41 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-06-29T18:27:41.668+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO AppInfoParser: Kafka version: 3.4.1
24/06/29 18:27:41 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2024-06-29T18:27:41.669+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO AppInfoParser: Kafka startTimeMs: 1719685661666
[2024-06-29T18:27:41.877+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/sources/0/0 using temp file file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/sources/0/.0.1611b0e8-94c6-494d-8757-c015907e92d8.tmp
[2024-06-29T18:27:41.888+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/sources/0/.0.1611b0e8-94c6-494d-8757-c015907e92d8.tmp to file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/sources/0/0
[2024-06-29T18:27:41.889+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO KafkaMicroBatchStream: Initial offsets: {"current_headlines":{"0":0}}
[2024-06-29T18:27:41.904+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/offsets/0 using temp file file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/offsets/.0.b0185700-206c-48a7-8f60-ae88595df64a.tmp
[2024-06-29T18:27:41.922+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/offsets/.0.b0185700-206c-48a7-8f60-ae88595df64a.tmp to file:/tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0/offsets/0
[2024-06-29T18:27:41.923+0000] {docker.py:413} INFO - 24/06/29 18:27:41 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719685661898,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-29T18:27:42.144+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-29T18:27:42.181+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-29T18:27:42.213+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-29T18:27:42.214+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-29T18:27:42.395+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO CodeGenerator: Code generated in 100.281416 ms
[2024-06-29T18:27:42.466+0000] {docker.py:413} INFO - 2024-06-29 18:27:42,465:wait_for_commands:INFO:Python Server ready to receive messages
2024-06-29 18:27:42,466:wait_for_commands:INFO:Received command c on object id p0
[2024-06-29T18:27:42.656+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO CodeGenerator: Code generated in 38.417 ms
[2024-06-29T18:27:42.670+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO CodeGenerator: Code generated in 10.16025 ms
[2024-06-29T18:27:42.681+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO CodeGenerator: Code generated in 7.628083 ms
[2024-06-29T18:27:42.782+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO CodeGenerator: Code generated in 6.054583 ms
[2024-06-29T18:27:42.787+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO CodeGenerator: Code generated in 3.711459 ms
[2024-06-29T18:27:42.849+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-29T18:27:42.859+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Registering RDD 11 (start at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2024-06-29T18:27:42.864+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Registering RDD 7 (start at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2024-06-29T18:27:42.865+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 200 output partitions
[2024-06-29T18:27:42.866+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
[2024-06-29T18:27:42.866+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[2024-06-29T18:27:42.867+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[2024-06-29T18:27:42.869+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-29T18:27:42.900+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.0 KiB, free 434.4 MiB)
[2024-06-29T18:27:42.914+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.4 MiB)
[2024-06-29T18:27:42.915+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:44765 (size: 7.4 KiB, free: 434.4 MiB)
[2024-06-29T18:27:42.918+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
[2024-06-29T18:27:42.927+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-29T18:27:42.928+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-06-29T18:27:42.941+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-29T18:27:42.945+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 38.8 KiB, free 434.3 MiB)
[2024-06-29T18:27:42.945+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 434.3 MiB)
[2024-06-29T18:27:42.946+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:44765 (size: 15.2 KiB, free: 434.4 MiB)
[2024-06-29T18:27:42.946+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2024-06-29T18:27:42.947+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/29 18:27:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-06-29T18:27:42.955+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 9967 bytes)
[2024-06-29T18:27:42.964+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
24/06/29 18:27:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11066 bytes)
[2024-06-29T18:27:42.965+0000] {docker.py:413} INFO - 24/06/29 18:27:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-06-29T18:27:43.037+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO CodeGenerator: Code generated in 16.119791 ms
[2024-06-29T18:27:43.048+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO CodeGenerator: Code generated in 5.95825 ms
[2024-06-29T18:27:43.048+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO CodeGenerator: Code generated in 10.257875 ms
[2024-06-29T18:27:43.055+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO CodeGenerator: Code generated in 4.597666 ms
[2024-06-29T18:27:43.058+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO CodeGenerator: Code generated in 5.112625 ms
[2024-06-29T18:27:43.067+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=current_headlines-0 fromOffset=0 untilOffset=658, for query queryId=2e07d8e7-d7b8-4abe-9b70-b84cfffebb7f batchId=0 taskId=1 partitionId=0
[2024-06-29T18:27:43.080+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO JDBCRDD: closed connection
[2024-06-29T18:27:43.096+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO CodeGenerator: Code generated in 7.017417 ms
[2024-06-29T18:27:43.105+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1986 bytes result sent to driver
[2024-06-29T18:27:43.112+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO CodeGenerator: Code generated in 9.751375 ms
[2024-06-29T18:27:43.115+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 166 ms on localhost (executor driver) (1/1)
[2024-06-29T18:27:43.123+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-06-29T18:27:43.132+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: ShuffleMapStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.253 s
[2024-06-29T18:27:43.133+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: looking for newly runnable stages
[2024-06-29T18:27:43.136+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
[2024-06-29T18:27:43.136+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2024-06-29T18:27:43.137+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: failed: Set()
[2024-06-29T18:27:43.140+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-29T18:27:43.166+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO AppInfoParser: Kafka version: 3.4.1
24/06/29 18:27:43 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2024-06-29T18:27:43.166+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO AppInfoParser: Kafka startTimeMs: 1719685663165
[2024-06-29T18:27:43.167+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Assigned to partition(s): current_headlines-0
[2024-06-29T18:27:43.172+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Seeking to offset 0 for partition current_headlines-0
[2024-06-29T18:27:43.177+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Resetting the last seen epoch of partition current_headlines-0 to 8 since the associated topicId changed from null to oTQJXETvSIybCiV1hWStYA
[2024-06-29T18:27:43.177+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Cluster ID: G13gPvLWR7u_hUqWhAPD3A
[2024-06-29T18:27:43.203+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Seeking to earliest offset of partition current_headlines-0
[2024-06-29T18:27:43.206+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Resetting offset for partition current_headlines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-29T18:27:43.207+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Seeking to latest offset of partition current_headlines-0
[2024-06-29T18:27:43.207+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Resetting offset for partition current_headlines-0 to position FetchPosition{offset=658, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-29T18:27:43.346+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Seeking to offset 500 for partition current_headlines-0
[2024-06-29T18:27:43.349+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Seeking to earliest offset of partition current_headlines-0
[2024-06-29T18:27:43.851+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Resetting offset for partition current_headlines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-29T18:27:43.854+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Seeking to latest offset of partition current_headlines-0
[2024-06-29T18:27:43.854+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Resetting offset for partition current_headlines-0 to position FetchPosition{offset=658, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=8}}.
[2024-06-29T18:27:43.884+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO KafkaDataConsumer: From Kafka topicPartition=current_headlines-0 groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor read 658 records through 2 polls (polled  out 658 records), taking 541735084 nanos, during time span of 713942875 nanos.
[2024-06-29T18:27:43.886+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2401 bytes result sent to driver
[2024-06-29T18:27:43.888+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 925 ms on localhost (executor driver) (1/1)
[2024-06-29T18:27:43.889+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-06-29T18:27:43.889+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: ShuffleMapStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 0.946 s
[2024-06-29T18:27:43.890+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: looking for newly runnable stages
24/06/29 18:27:43 INFO DAGScheduler: running: Set()
24/06/29 18:27:43 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2024-06-29T18:27:43.890+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: failed: Set()
[2024-06-29T18:27:43.890+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[18] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-29T18:27:43.917+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 72.4 KiB, free 434.3 MiB)
[2024-06-29T18:27:43.918+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.7 KiB, free 434.2 MiB)
[2024-06-29T18:27:43.919+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:44765 (size: 31.7 KiB, free: 434.3 MiB)
[2024-06-29T18:27:43.919+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
[2024-06-29T18:27:43.921+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[18] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-06-29T18:27:43.921+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
[2024-06-29T18:27:43.927+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 2) (localhost, executor driver, partition 1, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:43.928+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 3) (localhost, executor driver, partition 2, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:43.930+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 4) (localhost, executor driver, partition 3, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:43.934+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 5) (localhost, executor driver, partition 5, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:43.935+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 6) (localhost, executor driver, partition 6, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:43.936+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 7) (localhost, executor driver, partition 11, NODE_LOCAL, 10414 bytes) 
24/06/29 18:27:43 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 8) (localhost, executor driver, partition 12, NODE_LOCAL, 10414 bytes) 
24/06/29 18:27:43 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 9) (localhost, executor driver, partition 14, NODE_LOCAL, 10414 bytes) 
24/06/29 18:27:43 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 10) (localhost, executor driver, partition 15, NODE_LOCAL, 10414 bytes) 
24/06/29 18:27:43 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 11) (localhost, executor driver, partition 16, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:43.937+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 2.0 in stage 2.0 (TID 3)
[2024-06-29T18:27:43.943+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 1.0 in stage 2.0 (TID 2)
[2024-06-29T18:27:43.945+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 3.0 in stage 2.0 (TID 4)
[2024-06-29T18:27:43.946+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 6.0 in stage 2.0 (TID 6)
[2024-06-29T18:27:43.952+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 5.0 in stage 2.0 (TID 5)
[2024-06-29T18:27:43.953+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 11.0 in stage 2.0 (TID 7)
[2024-06-29T18:27:43.954+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 12.0 in stage 2.0 (TID 8)
[2024-06-29T18:27:43.954+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 14.0 in stage 2.0 (TID 9)
[2024-06-29T18:27:43.955+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 15.0 in stage 2.0 (TID 10)
[2024-06-29T18:27:43.959+0000] {docker.py:413} INFO - 24/06/29 18:27:43 INFO Executor: Running task 16.0 in stage 2.0 (TID 11)
[2024-06-29T18:27:44.039+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.041+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1271.0 B) non-empty blocks including 1 (1271.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1051.0 B) non-empty blocks including 1 (1051.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.042+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
[2024-06-29T18:27:44.042+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2024-06-29T18:27:44.043+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2024-06-29T18:27:44.043+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2024-06-29T18:27:44.043+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.043+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2024-06-29T18:27:44.043+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 12 ms
[2024-06-29T18:27:44.044+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 13 ms
[2024-06-29T18:27:44.049+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO CodeGenerator: Code generated in 5.948792 ms
[2024-06-29T18:27:44.062+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO CodeGenerator: Code generated in 8.306583 ms
[2024-06-29T18:27:44.081+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO CodeGenerator: Code generated in 11.231959 ms
[2024-06-29T18:27:44.087+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.089+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.089+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.090+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.091+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.091+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.092+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.094+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO CodeGenerator: Code generated in 6.552542 ms
[2024-06-29T18:27:44.111+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO CodeGenerator: Code generated in 9.719584 ms
[2024-06-29T18:27:44.627+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO CodeGenerator: Code generated in 5.803 ms
[2024-06-29T18:27:44.650+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Finished task 2.0 in stage 2.0 (TID 3). 6218 bytes result sent to driver
24/06/29 18:27:44 INFO Executor: Finished task 3.0 in stage 2.0 (TID 4). 6218 bytes result sent to driver
24/06/29 18:27:44 INFO Executor: Finished task 16.0 in stage 2.0 (TID 11). 6218 bytes result sent to driver
24/06/29 18:27:44 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 12) (localhost, executor driver, partition 17, NODE_LOCAL, 10414 bytes) 
24/06/29 18:27:44 INFO Executor: Running task 17.0 in stage 2.0 (TID 12)
24/06/29 18:27:44 INFO Executor: Finished task 6.0 in stage 2.0 (TID 6). 6218 bytes result sent to driver
24/06/29 18:27:44 INFO Executor: Finished task 15.0 in stage 2.0 (TID 10). 6218 bytes result sent to driver
24/06/29 18:27:44 INFO Executor: Finished task 1.0 in stage 2.0 (TID 2). 6218 bytes result sent to driver
24/06/29 18:27:44 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 13) (localhost, executor driver, partition 19, NODE_LOCAL, 10414 bytes) 
24/06/29 18:27:44 INFO Executor: Running task 19.0 in stage 2.0 (TID 13)
24/06/29 18:27:44 INFO Executor: Finished task 5.0 in stage 2.0 (TID 5). 6218 bytes result sent to driver
[2024-06-29T18:27:44.652+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 3) in 723 ms on localhost (executor driver) (1/200)
24/06/29 18:27:44 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 4) in 724 ms on localhost (executor driver) (2/200)
[2024-06-29T18:27:44.653+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 14) (localhost, executor driver, partition 22, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.655+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 22.0 in stage 2.0 (TID 14)
[2024-06-29T18:27:44.660+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1156.0 B) non-empty blocks including 1 (1156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.661+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2024-06-29T18:27:44.663+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 15) (localhost, executor driver, partition 25, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.663+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 16) (localhost, executor driver, partition 26, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.664+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 26.0 in stage 2.0 (TID 16)
24/06/29 18:27:44 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 11) in 727 ms on localhost (executor driver) (3/200)
[2024-06-29T18:27:44.664+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 6) in 728 ms on localhost (executor driver) (4/200)
24/06/29 18:27:44 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 10) in 727 ms on localhost (executor driver) (5/200)
[2024-06-29T18:27:44.672+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1271.0 B) non-empty blocks including 1 (1271.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/06/29 18:27:44 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 17) (localhost, executor driver, partition 27, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.687+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 18) (localhost, executor driver, partition 30, NODE_LOCAL, 10414 bytes) 
24/06/29 18:27:44 INFO Executor: Running task 30.0 in stage 2.0 (TID 18)
24/06/29 18:27:44 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 2) in 751 ms on localhost (executor driver) (6/200)
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.691+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 25.0 in stage 2.0 (TID 15)
[2024-06-29T18:27:44.692+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 27.0 in stage 2.0 (TID 17)
24/06/29 18:27:44 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 5) in 750 ms on localhost (executor driver) (7/200)
[2024-06-29T18:27:44.692+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (334.0 B) non-empty blocks including 1 (334.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.692+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.693+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (445.0 B) non-empty blocks including 1 (445.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.693+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.694+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (955.0 B) non-empty blocks including 1 (955.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1692.0 B) non-empty blocks including 1 (1692.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2024-06-29T18:27:44.701+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.702+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.703+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.703+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.708+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Finished task 27.0 in stage 2.0 (TID 17). 6175 bytes result sent to driver
[2024-06-29T18:27:44.709+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Finished task 26.0 in stage 2.0 (TID 16). 6175 bytes result sent to driver
[2024-06-29T18:27:44.710+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 19) (localhost, executor driver, partition 32, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.710+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 32.0 in stage 2.0 (TID 19)
[2024-06-29T18:27:44.711+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 20) (localhost, executor driver, partition 35, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.711+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 35.0 in stage 2.0 (TID 20)
24/06/29 18:27:44 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 17) in 52 ms on localhost (executor driver) (8/200)
[2024-06-29T18:27:44.712+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 16) in 56 ms on localhost (executor driver) (9/200)
[2024-06-29T18:27:44.714+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (955.0 B) non-empty blocks including 1 (955.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (868.0 B) non-empty blocks including 1 (868.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.715+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.715+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.717+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.725+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Finished task 32.0 in stage 2.0 (TID 19). 6175 bytes result sent to driver
24/06/29 18:27:44 INFO Executor: Finished task 35.0 in stage 2.0 (TID 20). 6175 bytes result sent to driver
[2024-06-29T18:27:44.726+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 21) (localhost, executor driver, partition 36, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.727+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 22) (localhost, executor driver, partition 39, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.728+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 36.0 in stage 2.0 (TID 21)
24/06/29 18:27:44 INFO Executor: Running task 39.0 in stage 2.0 (TID 22)
24/06/29 18:27:44 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 19) in 19 ms on localhost (executor driver) (10/200)
[2024-06-29T18:27:44.729+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 20) in 18 ms on localhost (executor driver) (11/200)
[2024-06-29T18:27:44.732+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.733+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.737+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.738+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.739+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.739+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.745+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Finished task 39.0 in stage 2.0 (TID 22). 6175 bytes result sent to driver
24/06/29 18:27:44 INFO Executor: Finished task 36.0 in stage 2.0 (TID 21). 6175 bytes result sent to driver
[2024-06-29T18:27:44.748+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 23) (localhost, executor driver, partition 40, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.750+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 24) (localhost, executor driver, partition 41, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.750+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 40.0 in stage 2.0 (TID 23)
24/06/29 18:27:44 INFO Executor: Running task 41.0 in stage 2.0 (TID 24)
[2024-06-29T18:27:44.752+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 22) in 18 ms on localhost (executor driver) (12/200)
24/06/29 18:27:44 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 21) in 20 ms on localhost (executor driver) (13/200)
[2024-06-29T18:27:44.752+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1051.0 B) non-empty blocks including 1 (1051.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.753+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.753+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (955.0 B) non-empty blocks including 1 (955.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:44765 in memory (size: 15.2 KiB, free: 434.4 MiB)
[2024-06-29T18:27:44.754+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.755+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.758+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:44765 in memory (size: 7.4 KiB, free: 434.4 MiB)
[2024-06-29T18:27:44.789+0000] {docker.py:413} INFO - 24/06/29 18:27:44 ERROR Executor: Exception in task 11.0 in stage 2.0 (TID 7)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T21:06:07Z','{"id":"al-jazeera-english","name":"Al Jazeera English"}','Al Jazeera','Troops begin to withdraw as Bolivian president rallies against coup attempt - Al Jazeera English','Regional organisations mobilised behind Bolivian government after troops and armoured vehicles gathered in the capital.','https://www.aljazeera.com/news/2024/6/26/fears-of-coup-attempt-in-bolivia-as-soldiers-storm-presidential-palace','https://www.aljazeera.com/wp-content/uploads/2024/06/AP24178775725792-1719438488.jpg?resize=1200%2C675','Bolivian President Luis Arce has offered his thanks to the people of the country after facing down a coup attempt that drew international condemnation and saw soldiers smash through the doors of the ? [+4381 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T21:06:07Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T21:06:07Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.789+0000] {docker.py:413} INFO - 24/06/29 18:27:44 ERROR Executor: Exception in task 30.0 in stage 2.0 (TID 18)
java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
24/06/29 18:27:44 ERROR Executor: Exception in task 25.0 in stage 2.0 (TID 15)
java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T19:20:00Z','{"id":"cnn","name":"CNN"}','Taylor Nicioli','Decades after the famed Kyrenia shipwreck?s discovery, researchers have a new estimate of when it sank - CNN','The timeline of a Hellenistic Kyrenia shipwreck stumped researchers for decades. But thanks to a cache of ancient almonds, a new study may have a better estimate.','https://www.cnn.com/2024/06/26/science/kyrenia-shipwreck-radiocarbon-dating-almonds-scn/index.html','https://media.cnn.com/api/v1/images/stellar/prod/kyrenia-ship-hull-during-excavation.jpg?c=16x9&q=w_800,c_fill','Sign up for CNNs Wonder Theory science newsletter.?Explore the universe with news on fascinating discoveries, scientific advancements and more.
A lone diverfirst laid eyes on the ancient Kyrenia shi? [+6426 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T19:20:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T19:20:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
24/06/29 18:27:44 ERROR Executor: Exception in task 40.0 in stage 2.0 (TID 23)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T14:14:28Z','{"id":null,"name":"BBC News"}',NULL,'Mishal Husain: How I?ll referee BBC leaders'' debate with voters at its heart - BBC.com','Ahead of the BBC debate, the Today programme presenter explains how she will ensure questions are answered.','https://www.bbc.com/news/articles/c6pp2kyg2k6o','https://ichef.bbci.co.uk/news/1024/branded_news/7794/live/71cb0130-3304-11ef-9f4c-8785b0c18838.png','On Wednesday night, on a debate stage built in an atrium at Nottingham Trent University, the two men who want to lead the country will face each other directly for the last time in this election camp? [+2359 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T14:14:28Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T14:14:28Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
24/06/29 18:27:44 ERROR Executor: Exception in task 14.0 in stage 2.0 (TID 9)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T04:40:45Z','{"id":"cbs-news","name":"CBS News"}','Jesse Sarles, Austen Erblat','Lauren Boebert wins crowded House GOP primary in new Colorado district - CBS News','Republican Rep. Lauren Boebert declared victory on Tuesday night in the primary election in her new Colorado district.','https://www.cbsnews.com/colorado/news/lauren-boebert-wins-colorado-4th-congressional-district-republican-primary/','https://assets2.cbsnewsstatic.com/hub/i/r/2024/06/26/7e070f25-2f00-46f7-b7c8-670413a2d960/thumbnail/1200x630/422748044f79ba0d00e98d2723e5f785/boebert.jpg?v=d44ea471ad55b1f821a0763c85064960','Republican Rep. Lauren Boebert has emerged victorious in the primary election in her new Colorado district.
The controversial Congresswoman defeated five GOP opponents in a competitive primary in Co? [+5329 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T04:40:45Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T04:40:45Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
24/06/29 18:27:44 ERROR Executor: Exception in task 22.0 in stage 2.0 (TID 14)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:36:40Z','{"id":"politico","name":"Politico"}','POLITICO','Trump debate playbook: Skip mock debates, fundraise and undermine the results - POLITICO',NULL,'https://www.politico.com/news/2024/06/26/trump-debate-preparation-00165187',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:36:40Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:36:40Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.790+0000] {docker.py:413} INFO - 24/06/29 18:27:44 ERROR Executor: Exception in task 19.0 in stage 2.0 (TID 13)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.790+0000] {docker.py:413} INFO - 24/06/29 18:27:44 ERROR Executor: Exception in task 41.0 in stage 2.0 (TID 24)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:44:00Z','{"id":"associated-press","name":"Associated Press"}','LARRY NEUMEISTER','Former Honduran President sentenced to 45 years in US prison - The Associated Press','Former Honduran President Juan Orlando Hern?ndez has been sentenced to 45 years in prison and fined $8 million for enabling drug traffickers to use his military and national police force to help get tons of cocaine into the United States. The 55-year-old was ?','https://apnews.com/article/honduras-president-juan-orlando-hernandez-corruption-3f98be974c58bb8a1b492108c4a7f297','https://dims.apnews.com/dims4/default/4c48030/2147483647/strip/true/crop/5760x3240+0+300/resize/1440x810!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F87%2Fd8%2Fe5c7def9831299395779f42be2f6%2F4d1cc18e0f644c768cb5e46583f3bc55','NEW YORK (AP) A defiant former Honduran President Juan Orlando Hern?ndez was sentenced in New York Wednesday to 45 years in prison for teaming up with some bribe-paying drug traffickers for over a de? [+5370 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:44:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:44:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
24/06/29 18:27:44 ERROR Executor: Exception in task 17.0 in stage 2.0 (TID 12)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T05:46:56Z','{"id":null,"name":"NPR"}',NULL,'FDA warns top U.S. bakery not to claim foods contain allergens when they don''t - NPR','The FDA found Bimbo Bakeries USA ? which includes brands such as Sara Lee and Ball Park buns and rolls ? listed ingredients such as sesame or tree nuts on labels even when they weren''t in the foods.','https://www.npr.org/2024/06/26/g-s1-6238/fda-warns-bakery-foods-allergens','https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2000x1125+0+104/resize/1400/quality/100/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F16%2Fe55fa0fb497f9e722a56bbbc2b8f%2Fap24177785376325.jpg','Federal food safety regulators said Tuesday that they have warned a top U.S. bakery to stop using labels that say its products contain potentially dangerous allergens when they don''t.
U.S. Food and ? [+2568 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T05:46:56Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T05:46:56Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.796+0000] {docker.py:413} INFO - 24/06/29 18:27:44 ERROR Executor: Exception in task 12.0 in stage 2.0 (TID 8)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T17:00:00Z','{"id":"the-verge","name":"The Verge"}','Jay Peters','An AI version of Al Michaels will deliver Olympic recaps on Peacock - The Verge','Peacock?s Paris Olympics coverage will include personalized daily recaps narrated by an AI-generated version of Al Michaels? voice.','https://www.theverge.com/2024/6/26/24185774/olympics-ai-al-michaels-voice-recaps','https://cdn.vox-cdn.com/thumbor/loA3E-uE5D5ufnZcSYeCltrB99o=/0x0:8640x5760/1200x628/filters:focal(5224x2847:5225x2848)/cdn.vox-cdn.com/uploads/chorus_asset/file/25505642/1772478978.jpg','An AI version of Al Michaels will deliver Olympic recaps on Peacock
An AI version of Al Michaels will deliver Olympic recaps on Peacock
 / NBCs AI-generated voice could create nearly 7 million cust? [+2697 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T17:00:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T17:00:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.809+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 25) (localhost, executor driver, partition 42, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.809+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 42.0 in stage 2.0 (TID 25)
[2024-06-29T18:27:44.811+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 26) (localhost, executor driver, partition 44, NODE_LOCAL, 10414 bytes) 
24/06/29 18:27:44 INFO Executor: Running task 44.0 in stage 2.0 (TID 26)
[2024-06-29T18:27:44.811+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 45.0 in stage 2.0 (TID 27) (localhost, executor driver, partition 45, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.811+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 45.0 in stage 2.0 (TID 27)
[2024-06-29T18:27:44.812+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 46.0 in stage 2.0 (TID 28) (localhost, executor driver, partition 46, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.812+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 46.0 in stage 2.0 (TID 28)
[2024-06-29T18:27:44.812+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 48.0 in stage 2.0 (TID 29) (localhost, executor driver, partition 48, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.825+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Starting task 49.0 in stage 2.0 (TID 30) (localhost, executor driver, partition 49, NODE_LOCAL, 10414 bytes)
[2024-06-29T18:27:44.827+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 49.0 in stage 2.0 (TID 30)
[2024-06-29T18:27:44.828+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Running task 48.0 in stage 2.0 (TID 29)
[2024-06-29T18:27:44.828+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.829+0000] {docker.py:413} INFO - 24/06/29 18:27:44 ERROR TaskSetManager: Task 19 in stage 2.0 failed 1 times; aborting job
[2024-06-29T18:27:44.834+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (868.0 B) non-empty blocks including 1 (868.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.835+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.835+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.836+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 22.0 in stage 2.0 (TID 14) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:36:40Z','{"id":"politico","name":"Politico"}','POLITICO','Trump debate playbook: Skip mock debates, fundraise and undermine the results - POLITICO',NULL,'https://www.politico.com/news/2024/06/26/trump-debate-preparation-00165187',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:36:40Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:36:40Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.836+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 11.0 in stage 2.0 (TID 7) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T21:06:07Z','{"id":"al-jazeera-english","name":"Al Jazeera English"}','Al Jazeera','Troops begin to withdraw as Bolivian president rallies against coup attempt - Al Jazeera English','Regional organisations mobilised behind Bolivian government after troops and armoured vehicles gathered in the capital.','https://www.aljazeera.com/news/2024/6/26/fears-of-coup-attempt-in-bolivia-as-soldiers-storm-presidential-palace','https://www.aljazeera.com/wp-content/uploads/2024/06/AP24178775725792-1719438488.jpg?resize=1200%2C675','Bolivian President Luis Arce has offered his thanks to the people of the country after facing down a coup attempt that drew international condemnation and saw soldiers smash through the doors of the ? [+4381 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T21:06:07Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T21:06:07Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.837+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 40.0 in stage 2.0 (TID 23) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T14:14:28Z','{"id":null,"name":"BBC News"}',NULL,'Mishal Husain: How I?ll referee BBC leaders'' debate with voters at its heart - BBC.com','Ahead of the BBC debate, the Today programme presenter explains how she will ensure questions are answered.','https://www.bbc.com/news/articles/c6pp2kyg2k6o','https://ichef.bbci.co.uk/news/1024/branded_news/7794/live/71cb0130-3304-11ef-9f4c-8785b0c18838.png','On Wednesday night, on a debate stage built in an atrium at Nottingham Trent University, the two men who want to lead the country will face each other directly for the last time in this election camp? [+2359 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T14:14:28Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T14:14:28Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.838+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 41.0 in stage 2.0 (TID 24) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:44:00Z','{"id":"associated-press","name":"Associated Press"}','LARRY NEUMEISTER','Former Honduran President sentenced to 45 years in US prison - The Associated Press','Former Honduran President Juan Orlando Hern?ndez has been sentenced to 45 years in prison and fined $8 million for enabling drug traffickers to use his military and national police force to help get tons of cocaine into the United States. The 55-year-old was ?','https://apnews.com/article/honduras-president-juan-orlando-hernandez-corruption-3f98be974c58bb8a1b492108c4a7f297','https://dims.apnews.com/dims4/default/4c48030/2147483647/strip/true/crop/5760x3240+0+300/resize/1440x810!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F87%2Fd8%2Fe5c7def9831299395779f42be2f6%2F4d1cc18e0f644c768cb5e46583f3bc55','NEW YORK (AP) A defiant former Honduran President Juan Orlando Hern?ndez was sentenced in New York Wednesday to 45 years in prison for teaming up with some bribe-paying drug traffickers for over a de? [+5370 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:44:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:44:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.841+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.843+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.845+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.845+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 WARN TaskSetManager: Lost task 14.0 in stage 2.0 (TID 9) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T04:40:45Z','{"id":"cbs-news","name":"CBS News"}','Jesse Sarles, Austen Erblat','Lauren Boebert wins crowded House GOP primary in new Colorado district - CBS News','Republican Rep. Lauren Boebert declared victory on Tuesday night in the primary election in her new Colorado district.','https://www.cbsnews.com/colorado/news/lauren-boebert-wins-colorado-4th-congressional-district-republican-primary/','https://assets2.cbsnewsstatic.com/hub/i/r/2024/06/26/7e070f25-2f00-46f7-b7c8-670413a2d960/thumbnail/1200x630/422748044f79ba0d00e98d2723e5f785/boebert.jpg?v=d44ea471ad55b1f821a0763c85064960','Republican Rep. Lauren Boebert has emerged victorious in the primary election in her new Colorado district.
The controversial Congresswoman defeated five GOP opponents in a competitive primary in Co? [+5329 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T04:40:45Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T04:40:45Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.846+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.847+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.850+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.851+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.851+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2024-06-29T18:27:44.851+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.852+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.852+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSchedulerImpl: Cancelling stage 2
[2024-06-29T18:27:44.852+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-06-29T18:27:44.853+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.854+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2024-06-29T18:27:44.856+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T18:27:44.857+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T18:27:44.858+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 18:27:44 INFO Executor: Executor is trying to kill task 49.0 in stage 2.0 (TID 30), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 INFO Executor: Executor is trying to kill task 45.0 in stage 2.0 (TID 27), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 INFO Executor: Executor is trying to kill task 46.0 in stage 2.0 (TID 28), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 INFO Executor: Executor is trying to kill task 42.0 in stage 2.0 (TID 25), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 INFO Executor: Executor is trying to kill task 48.0 in stage 2.0 (TID 29), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 INFO Executor: Executor is trying to kill task 44.0 in stage 2.0 (TID 26), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 INFO TaskSchedulerImpl: Stage 2 was cancelled
24/06/29 18:27:44 INFO Executor: Finished task 46.0 in stage 2.0 (TID 28). 6175 bytes result sent to driver
24/06/29 18:27:44 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) failed in 0.940 s due to Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 INFO Executor: Executor interrupted and killed task 42.0 in stage 2.0 (TID 25), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 WARN TaskSetManager: Lost task 25.0 in stage 2.0 (TID 15) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T19:20:00Z','{"id":"cnn","name":"CNN"}','Taylor Nicioli','Decades after the famed Kyrenia shipwreck?s discovery, researchers have a new estimate of when it sank - CNN','The timeline of a Hellenistic Kyrenia shipwreck stumped researchers for decades. But thanks to a cache of ancient almonds, a new study may have a better estimate.','https://www.cnn.com/2024/06/26/science/kyrenia-shipwreck-radiocarbon-dating-almonds-scn/index.html','https://media.cnn.com/api/v1/images/stellar/prod/kyrenia-ship-hull-during-excavation.jpg?c=16x9&q=w_800,c_fill','Sign up for CNNs Wonder Theory science newsletter.?Explore the universe with news on fascinating discoveries, scientific advancements and more.
A lone diverfirst laid eyes on the ancient Kyrenia shi? [+6426 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T19:20:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T19:20:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

24/06/29 18:27:44 WARN TaskSetManager: Lost task 17.0 in stage 2.0 (TID 12) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","conten
[2024-06-29T18:27:44.858+0000] {docker.py:413} INFO - t") VALUES ('2024-06-26T05:46:56Z','{"id":null,"name":"NPR"}',NULL,'FDA warns top U.S. bakery not to claim foods contain allergens when they don''t - NPR','The FDA found Bimbo Bakeries USA ? which includes brands such as Sara Lee and Ball Park buns and rolls ? listed ingredients such as sesame or tree nuts on labels even when they weren''t in the foods.','https://www.npr.org/2024/06/26/g-s1-6238/fda-warns-bakery-foods-allergens','https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2000x1125+0+104/resize/1400/quality/100/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F16%2Fe55fa0fb497f9e722a56bbbc2b8f%2Fap24177785376325.jpg','Federal food safety regulators said Tuesday that they have warned a top U.S. bakery to stop using labels that say its products contain potentially dangerous allergens when they don''t.
U.S. Food and ? [+2568 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T05:46:56Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T05:46:56Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

24/06/29 18:27:44 INFO Executor: Executor killed task 45.0 in stage 2.0 (TID 27), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 18:27:44 WARN TaskSetManager: Lost task 12.0 in stage 2.0 (TID 8) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T17:00:00Z','{"id":"the-verge","name":"The Verge"}','Jay Peters','An AI version of Al Michaels will deliver Olympic recaps on Peacock - The Verge','Peacock?s Paris Olympics coverage will include personalized daily recaps narrated by an AI-generated version of Al Michaels? voice.','https://www.theverge.com/2024/6/26/24185774/olympics-ai-al-michaels-voice-recaps','https://cdn.vox-cdn.com/thumbor/loA3E-uE5D5ufnZcSYeCltrB99o=/0x0:8640x5760/1200x628/filters:focal(5224x2847:5225x2848)/cdn.vox-cdn.com/uploads/chorus_asset/file/25505642/1772478978.jpg','An AI version of Al Michaels will deliver Olympic recaps on Peacock
An AI version of Al Michaels will deliver Olympic recaps on Peacock
 / NBCs AI-generated voice could create nearly 7 million cust? [+2697 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T17:00:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T17:00:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:44.859+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO TaskSetManager: Finished task 46.0 in stage 2.0 (TID 28) in 40 ms on localhost (executor driver) (14/200)
[2024-06-29T18:27:44.859+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 42.0 in stage 2.0 (TID 25) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
[2024-06-29T18:27:44.859+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Executor killed task 49.0 in stage 2.0 (TID 30), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-06-29T18:27:44.860+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Executor killed task 44.0 in stage 2.0 (TID 26), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-06-29T18:27:44.861+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO DAGScheduler: Job 0 failed: start at NativeMethodAccessorImpl.java:0, took 2.007676 s
24/06/29 18:27:44 WARN TaskSetManager: Lost task 45.0 in stage 2.0 (TID 27) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
[2024-06-29T18:27:44.861+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 44.0 in stage 2.0 (TID 26) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
[2024-06-29T18:27:44.864+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 49.0 in stage 2.0 (TID 30) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
[2024-06-29T18:27:44.878+0000] {docker.py:413} INFO - 24/06/29 18:27:44 INFO Executor: Executor killed task 48.0 in stage 2.0 (TID 29), reason: Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-06-29T18:27:44.880+0000] {docker.py:413} INFO - 24/06/29 18:27:44 WARN TaskSetManager: Lost task 48.0 in stage 2.0 (TID 29) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
24/06/29 18:27:44 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-06-29T18:27:45.063+0000] {docker.py:413} INFO - 2024-06-29 18:27:45,009:_call_proxy:ERROR:There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 83, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche… [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.
[2024-06-29T18:27:45.065+0000] {docker.py:413} INFO - QueryExe
[2024-06-29T18:27:45.065+0000] {docker.py:413} INFO - cution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche… [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:45.110+0000] {docker.py:413} INFO - 24/06/29 18:27:45 ERROR MicroBatchExecution: Query [id = 2e07d8e7-d7b8-4abe-9b70-b84cfffebb7f, runId = e244d99d-3e59-47e1-a09f-40b7cca665c8] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 83, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	a
[2024-06-29T18:27:45.111+0000] {docker.py:413} INFO - t org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more


	at py4j.Protocol.getReturnValue(Protocol.java:476)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2024-06-29T18:27:45.112+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2024-06-29T18:27:45.112+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO Metrics: Metrics scheduler closed
24/06/29 18:27:45 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-29T18:27:45.113+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO Metrics: Metrics reporters closed
[2024-06-29T18:27:45.114+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO MicroBatchExecution: Async log purge executor pool for query [id = 2e07d8e7-d7b8-4abe-9b70-b84cfffebb7f, runId = e244d99d-3e59-47e1-a09f-40b7cca665c8] has been shutdown
[2024-06-29T18:27:45.312+0000] {docker.py:413} INFO - Traceback (most recent call last):
  File "/opt/bitnami/spark/spark_streaming.py", line 103, in <module>
[2024-06-29T18:27:45.312+0000] {docker.py:413} INFO - write_to_postgres()
  File "/opt/bitnami/spark/spark_streaming.py", line 99, in write_to_postgres
[2024-06-29T18:27:45.313+0000] {docker.py:413} INFO - start_streaming(df_final, spark=spark)
  File "/opt/bitnami/spark/spark_streaming.py", line 90, in start_streaming
[2024-06-29T18:27:45.313+0000] {docker.py:413} INFO - return query.awaitTermination()
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
[2024-06-29T18:27:45.313+0000] {docker.py:413} INFO - File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2024-06-29T18:27:45.314+0000] {docker.py:413} INFO - File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
pyspark.errors.exceptions.captured.StreamingQueryException
[2024-06-29T18:27:45.317+0000] {docker.py:413} INFO - : [STREAM_FAILED] Query [id = 2e07d8e7-d7b8-4abe-9b70-b84cfffebb7f, runId = e244d99d-3e59-47e1-a09f-40b7cca665c8] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 83, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 19 in stage 2.0 failed 1 times, most recent failure: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche… [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche… [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T18:27:45.318+0000] {docker.py:413} INFO - 2024-06-29 18:27:45,316:close:INFO:Closing down clientserver connection
[2024-06-29T18:27:45.342+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/29 18:27:45 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1, groupId=spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor] Request joining group due to: consumer pro-actively leaving the group
[2024-06-29T18:27:45.343+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO Metrics: Metrics scheduler closed
[2024-06-29T18:27:45.343+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-29T18:27:45.343+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO Metrics: Metrics reporters closed
[2024-06-29T18:27:45.343+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-7c2e561a-c1f7-4c79-9363-801404656021-1298021607-executor-1 unregistered
[2024-06-29T18:27:45.343+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO SparkContext: Invoking stop() from shutdown hook
[2024-06-29T18:27:45.344+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-06-29T18:27:45.351+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
[2024-06-29T18:27:45.356+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-06-29T18:27:45.365+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO MemoryStore: MemoryStore cleared
[2024-06-29T18:27:45.366+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO BlockManager: BlockManager stopped
[2024-06-29T18:27:45.367+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-06-29T18:27:45.369+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-06-29T18:27:45.378+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO SparkContext: Successfully stopped SparkContext
[2024-06-29T18:27:45.379+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO ShutdownHookManager: Shutdown hook called
[2024-06-29T18:27:45.379+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c
[2024-06-29T18:27:45.380+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-42714fa0-0d0c-4fa1-aa5f-0fd19eb47de3
[2024-06-29T18:27:45.381+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO ShutdownHookManager: Deleting directory /tmp/temporary-9bdd030a-5c33-4fee-9f16-5026e90ef8a0
[2024-06-29T18:27:45.384+0000] {docker.py:413} INFO - 24/06/29 18:27:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-737ebd33-d709-47c6-9b6c-095f0d490e4c/pyspark-52b3321a-a64e-49c8-84ae-2b657e8ded2c
[2024-06-29T18:27:45.596+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 486, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 360, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 421, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-06-29T18:27:45.599+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=kafka_spark_dag, task_id=pyspark_consumer, execution_date=20240629T182638, start_date=20240629T182715, end_date=20240629T182745
[2024-06-29T18:27:45.607+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 8 for task pyspark_consumer (Docker container failed: {'StatusCode': 1}; 909)
[2024-06-29T18:27:45.630+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-06-29T18:27:45.641+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check

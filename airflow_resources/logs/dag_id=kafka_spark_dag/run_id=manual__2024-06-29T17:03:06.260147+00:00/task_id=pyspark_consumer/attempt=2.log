[2024-06-29T17:04:45.444+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-06-29T17:03:06.260147+00:00 [queued]>
[2024-06-29T17:04:45.449+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-06-29T17:03:06.260147+00:00 [queued]>
[2024-06-29T17:04:45.450+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2024-06-29T17:04:45.456+0000] {taskinstance.py:1382} INFO - Executing <Task(DockerOperator): pyspark_consumer> on 2024-06-29 17:03:06.260147+00:00
[2024-06-29T17:04:45.460+0000] {standard_task_runner.py:57} INFO - Started process 65234 to run task
[2024-06-29T17:04:45.462+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'kafka_spark_dag', 'pyspark_consumer', 'manual__2024-06-29T17:03:06.260147+00:00', '--job-id', '218', '--raw', '--subdir', 'DAGS_FOLDER/dag_kafka_spark.py', '--cfg-path', '/tmp/tmpe40k2uay']
[2024-06-29T17:04:45.464+0000] {standard_task_runner.py:85} INFO - Job 218: Subtask pyspark_consumer
[2024-06-29T17:04:45.490+0000] {task_command.py:416} INFO - Running <TaskInstance: kafka_spark_dag.pyspark_consumer manual__2024-06-29T17:03:06.260147+00:00 [running]> on host cd176cdca805
[2024-06-29T17:04:45.532+0000] {taskinstance.py:1662} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='kafka_spark_dag' AIRFLOW_CTX_TASK_ID='pyspark_consumer' AIRFLOW_CTX_EXECUTION_DATE='2024-06-29T17:03:06.260147+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-06-29T17:03:06.260147+00:00'
[2024-06-29T17:04:45.597+0000] {docker.py:343} INFO - Starting docker container from image current-headlines/spark:latest
[2024-06-29T17:04:45.850+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m17:04:45.84 [0m[38;5;2mINFO [0m ==>
[2024-06-29T17:04:45.850+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m17:04:45.85 [0m[38;5;2mINFO [0m ==> [1mWelcome to the Bitnami spark container[0m
[2024-06-29T17:04:45.851+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m17:04:45.85 [0m[38;5;2mINFO [0m ==> Subscribe to project updates by watching [1mhttps://github.com/bitnami/containers[0m
[2024-06-29T17:04:45.852+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m17:04:45.85 [0m[38;5;2mINFO [0m ==> Submit issues and feature requests at [1mhttps://github.com/bitnami/containers/issues[0m
[2024-06-29T17:04:45.853+0000] {docker.py:413} INFO - [38;5;6mspark [38;5;5m17:04:45.85 [0m[38;5;2mINFO [0m ==>
[2024-06-29T17:04:45.858+0000] {docker.py:413} INFO - 
[2024-06-29T17:04:47.001+0000] {docker.py:413} INFO - :: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2024-06-29T17:04:47.036+0000] {docker.py:413} INFO - Ivy Default Cache set to: /opt/bitnami/spark/.ivy2/cache
The jars for the packages stored in: /opt/bitnami/spark/.ivy2/jars
[2024-06-29T17:04:47.038+0000] {docker.py:413} INFO - org.postgresql#postgresql added as a dependency
org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency
[2024-06-29T17:04:47.039+0000] {docker.py:413} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-c63c4436-f8d7-4ef7-8df0-65085efee999;1.0
	confs: [default]
[2024-06-29T17:04:48.610+0000] {docker.py:413} INFO - found org.postgresql#postgresql;42.5.4 in central
[2024-06-29T17:04:48.667+0000] {docker.py:413} INFO - found org.checkerframework#checker-qual;3.5.0 in central
[2024-06-29T17:04:49.884+0000] {docker.py:413} INFO - found org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 in central
[2024-06-29T17:04:50.064+0000] {docker.py:413} INFO - found org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 in central
[2024-06-29T17:04:50.124+0000] {docker.py:413} INFO - found org.apache.kafka#kafka-clients;3.4.1 in central
[2024-06-29T17:04:50.215+0000] {docker.py:413} INFO - found org.lz4#lz4-java;1.8.0 in central
[2024-06-29T17:04:50.275+0000] {docker.py:413} INFO - found org.xerial.snappy#snappy-java;1.1.10.3 in central
[2024-06-29T17:04:50.518+0000] {docker.py:413} INFO - found org.slf4j#slf4j-api;2.0.7 in central
[2024-06-29T17:04:50.936+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-runtime;3.3.4 in central
[2024-06-29T17:04:51.011+0000] {docker.py:413} INFO - found org.apache.hadoop#hadoop-client-api;3.3.4 in central
[2024-06-29T17:04:51.755+0000] {docker.py:413} INFO - found commons-logging#commons-logging;1.1.3 in central
[2024-06-29T17:04:51.812+0000] {docker.py:413} INFO - found com.google.code.findbugs#jsr305;3.0.0 in central
[2024-06-29T17:04:52.443+0000] {docker.py:413} INFO - found org.apache.commons#commons-pool2;2.11.1 in central
[2024-06-29T17:04:52.471+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.5.4/postgresql-42.5.4.jar ...
[2024-06-29T17:04:55.702+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.postgresql#postgresql;42.5.4!postgresql.jar (3251ms)
[2024-06-29T17:04:55.745+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.0/spark-sql-kafka-0-10_2.12-3.5.0.jar ...
[2024-06-29T17:04:58.243+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0!spark-sql-kafka-0-10_2.12.jar (2540ms)
[2024-06-29T17:04:58.267+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...
[2024-06-29T17:04:58.532+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (287ms)
[2024-06-29T17:04:58.554+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.0/spark-token-provider-kafka-0-10_2.12-3.5.0.jar ...
[2024-06-29T17:04:58.619+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0!spark-token-provider-kafka-0-10_2.12.jar (88ms)
[2024-06-29T17:04:58.638+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.1/kafka-clients-3.4.1.jar ...
[2024-06-29T17:05:09.402+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.kafka#kafka-clients;3.4.1!kafka-clients.jar (10781ms)
[2024-06-29T17:05:09.424+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...
[2024-06-29T17:05:09.477+0000] {docker.py:413} INFO - [SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (76ms)
[2024-06-29T17:05:09.501+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar ...
[2024-06-29T17:05:09.676+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.commons#commons-pool2;2.11.1!commons-pool2.jar (192ms)
[2024-06-29T17:05:09.727+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.3.4/hadoop-client-runtime-3.3.4.jar ...
[2024-06-29T17:05:33.577+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.3.4!hadoop-client-runtime.jar (23906ms)
[2024-06-29T17:05:33.612+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/lz4/lz4-java/1.8.0/lz4-java-1.8.0.jar ...
[2024-06-29T17:05:34.481+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.lz4#lz4-java;1.8.0!lz4-java.jar (902ms)
[2024-06-29T17:05:34.505+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.3/snappy-java-1.1.10.3.jar ...
[2024-06-29T17:05:36.697+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.3!snappy-java.jar(bundle) (2214ms)
[2024-06-29T17:05:36.737+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.7/slf4j-api-2.0.7.jar ...
[2024-06-29T17:05:36.786+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.slf4j#slf4j-api;2.0.7!slf4j-api.jar (88ms)
[2024-06-29T17:05:36.806+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.3.4/hadoop-client-api-3.3.4.jar ...
[2024-06-29T17:05:42.538+0000] {docker.py:413} INFO - [SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.3.4!hadoop-client-api.jar (5751ms)
[2024-06-29T17:05:42.558+0000] {docker.py:413} INFO - downloading https://repo1.maven.org/maven2/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar ...
[2024-06-29T17:05:42.590+0000] {docker.py:413} INFO - [SUCCESSFUL ] commons-logging#commons-logging;1.1.3!commons-logging.jar (50ms)
[2024-06-29T17:05:42.593+0000] {docker.py:413} INFO - :: resolution report :: resolve 5410ms :: artifacts dl 50141ms
[2024-06-29T17:05:42.594+0000] {docker.py:413} INFO - :: modules in use:
[2024-06-29T17:05:42.595+0000] {docker.py:413} INFO - com.google.code.findbugs#jsr305;3.0.0 from central in [default]
	commons-logging#commons-logging;1.1.3 from central in [default]
[2024-06-29T17:05:42.596+0000] {docker.py:413} INFO - org.apache.commons#commons-pool2;2.11.1 from central in [default]
	org.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]
	org.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]
[2024-06-29T17:05:42.597+0000] {docker.py:413} INFO - org.apache.kafka#kafka-clients;3.4.1 from central in [default]
	org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.0 from central in [default]
	org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.0 from central in [default]
[2024-06-29T17:05:42.597+0000] {docker.py:413} INFO - org.checkerframework#checker-qual;3.5.0 from central in [default]
[2024-06-29T17:05:42.598+0000] {docker.py:413} INFO - org.lz4#lz4-java;1.8.0 from central in [default]
	org.postgresql#postgresql;42.5.4 from central in [default]
[2024-06-29T17:05:42.598+0000] {docker.py:413} INFO - org.slf4j#slf4j-api;2.0.7 from central in [default]
	org.xerial.snappy#snappy-java;1.1.10.3 from central in [default]
[2024-06-29T17:05:42.599+0000] {docker.py:413} INFO - ---------------------------------------------------------------------
[2024-06-29T17:05:42.600+0000] {docker.py:413} INFO - |                  |            modules            ||   artifacts   |
[2024-06-29T17:05:42.600+0000] {docker.py:413} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
	---------------------------------------------------------------------
[2024-06-29T17:05:42.601+0000] {docker.py:413} INFO - |      default     |   13  |   13  |   13  |   0   ||   13  |   13  |
	---------------------------------------------------------------------
[2024-06-29T17:05:42.608+0000] {docker.py:413} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-c63c4436-f8d7-4ef7-8df0-65085efee999
[2024-06-29T17:05:42.609+0000] {docker.py:413} INFO - confs: [default]
[2024-06-29T17:05:42.669+0000] {docker.py:413} INFO - 13 artifacts copied, 0 already retrieved (58001kB/61ms)
[2024-06-29T17:05:42.780+0000] {docker.py:413} INFO - 24/06/29 17:05:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2024-06-29T17:05:43.426+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Running Spark version 3.5.0
[2024-06-29T17:05:43.427+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: OS info Linux, 6.6.12-linuxkit, aarch64
[2024-06-29T17:05:43.427+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Java version 17.0.10
[2024-06-29T17:05:43.439+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO ResourceUtils: ==============================================================
[2024-06-29T17:05:43.439+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO ResourceUtils: No custom resources configured for spark.driver.
[2024-06-29T17:05:43.439+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO ResourceUtils: ==============================================================
[2024-06-29T17:05:43.440+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Submitted application: PostgreSQL Connection with PySpark
[2024-06-29T17:05:43.452+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2024-06-29T17:05:43.456+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO ResourceProfile: Limiting resource is cpu
[2024-06-29T17:05:43.456+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2024-06-29T17:05:43.490+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SecurityManager: Changing view acls to: spark
[2024-06-29T17:05:43.490+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SecurityManager: Changing modify acls to: spark
[2024-06-29T17:05:43.490+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SecurityManager: Changing view acls groups to:
[2024-06-29T17:05:43.491+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SecurityManager: Changing modify acls groups to:
[2024-06-29T17:05:43.491+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: spark; groups with view permissions: EMPTY; users with modify permissions: spark; groups with modify permissions: EMPTY
[2024-06-29T17:05:43.601+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Successfully started service 'sparkDriver' on port 33871.
[2024-06-29T17:05:43.618+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkEnv: Registering MapOutputTracker
[2024-06-29T17:05:43.643+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkEnv: Registering BlockManagerMaster
[2024-06-29T17:05:43.645+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2024-06-29T17:05:43.645+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2024-06-29T17:05:43.648+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2024-06-29T17:05:43.662+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6620d780-c8c1-4fd9-a83a-9fcf1b90156e
[2024-06-29T17:05:43.671+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2024-06-29T17:05:43.680+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2024-06-29T17:05:43.757+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2024-06-29T17:05:43.792+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2024-06-29T17:05:43.812+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar at spark://localhost:33871/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1719680743421
[2024-06-29T17:05:43.812+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at spark://localhost:33871/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.813+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar at spark://localhost:33871/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.813+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at spark://localhost:33871/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.813+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at spark://localhost:33871/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719680743421
[2024-06-29T17:05:43.813+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://localhost:33871/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.814+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at spark://localhost:33871/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719680743421
24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at spark://localhost:33871/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719680743421
[2024-06-29T17:05:43.814+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at spark://localhost:33871/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.818+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at spark://localhost:33871/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719680743421
[2024-06-29T17:05:43.819+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at spark://localhost:33871/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719680743421
24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at spark://localhost:33871/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719680743421
24/06/29 17:05:43 INFO SparkContext: Added JAR file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://localhost:33871/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719680743421
[2024-06-29T17:05:43.819+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1719680743421
[2024-06-29T17:05:43.819+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.postgresql_postgresql-42.5.4.jar
[2024-06-29T17:05:43.821+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.822+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T17:05:43.824+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.824+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.checkerframework_checker-qual-3.5.0.jar
[2024-06-29T17:05:43.826+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.827+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T17:05:43.831+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719680743421
24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-06-29T17:05:43.839+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.839+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-29T17:05:43.842+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719680743421
24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-29T17:05:43.846+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719680743421
[2024-06-29T17:05:43.847+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-06-29T17:05:43.863+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar at file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.863+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.lz4_lz4-java-1.8.0.jar
[2024-06-29T17:05:43.865+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar at file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719680743421
[2024-06-29T17:05:43.865+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-06-29T17:05:43.868+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar at file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719680743421
[2024-06-29T17:05:43.868+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.slf4j_slf4j-api-2.0.7.jar
[2024-06-29T17:05:43.870+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar at file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719680743421
[2024-06-29T17:05:43.870+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-06-29T17:05:43.882+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO SparkContext: Added file file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719680743421
[2024-06-29T17:05:43.882+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: Copying /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/commons-logging_commons-logging-1.1.3.jar
[2024-06-29T17:05:43.920+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Starting executor ID driver on host localhost
[2024-06-29T17:05:43.920+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: OS info Linux, 6.6.12-linuxkit, aarch64
[2024-06-29T17:05:43.921+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Java version 17.0.10
[2024-06-29T17:05:43.924+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2024-06-29T17:05:43.924+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@13efe24b for default.
[2024-06-29T17:05:43.929+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.938+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T17:05:43.940+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719680743421
[2024-06-29T17:05:43.952+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-06-29T17:05:43.954+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719680743421
[2024-06-29T17:05:43.954+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.commons_commons-pool2-2.11.1.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-29T17:05:43.956+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1719680743421
[2024-06-29T17:05:43.956+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.postgresql_postgresql-42.5.4.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.postgresql_postgresql-42.5.4.jar
[2024-06-29T17:05:43.958+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719680743421
[2024-06-29T17:05:43.958+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.slf4j_slf4j-api-2.0.7.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.slf4j_slf4j-api-2.0.7.jar
[2024-06-29T17:05:43.960+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719680743421
[2024-06-29T17:05:43.962+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.kafka_kafka-clients-3.4.1.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-06-29T17:05:43.963+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.964+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-29T17:05:43.965+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.965+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.lz4_lz4-java-1.8.0.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.lz4_lz4-java-1.8.0.jar
[2024-06-29T17:05:43.967+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.967+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T17:05:43.969+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719680743421
[2024-06-29T17:05:43.976+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-06-29T17:05:43.977+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719680743421
[2024-06-29T17:05:43.977+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/commons-logging_commons-logging-1.1.3.jar
[2024-06-29T17:05:43.979+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:43.979+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.checkerframework_checker-qual-3.5.0.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.checkerframework_checker-qual-3.5.0.jar
[2024-06-29T17:05:43.980+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching file:///opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719680743421
[2024-06-29T17:05:43.981+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Utils: /opt/bitnami/spark/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-06-29T17:05:43.984+0000] {docker.py:413} INFO - 24/06/29 17:05:43 INFO Executor: Fetching spark://localhost:33871/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar with timestamp 1719680743421
[2024-06-29T17:05:44.007+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:33871 after 17 ms (0 ms spent in bootstraps)
[2024-06-29T17:05:44.013+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp1280319991586684312.tmp
[2024-06-29T17:05:44.101+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp1280319991586684312.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar
[2024-06-29T17:05:44.118+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.hadoop_hadoop-client-runtime-3.3.4.jar to class loader default
[2024-06-29T17:05:44.118+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar with timestamp 1719680743421
[2024-06-29T17:05:44.119+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.apache.hadoop_hadoop-client-api-3.3.4.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp14671935221209641390.tmp
[2024-06-29T17:05:44.155+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp14671935221209641390.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.hadoop_hadoop-client-api-3.3.4.jar
[2024-06-29T17:05:44.158+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.hadoop_hadoop-client-api-3.3.4.jar to class loader default
[2024-06-29T17:05:44.158+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.slf4j_slf4j-api-2.0.7.jar with timestamp 1719680743421
[2024-06-29T17:05:44.159+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.slf4j_slf4j-api-2.0.7.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp6743529925394984396.tmp
[2024-06-29T17:05:44.159+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp6743529925394984396.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.slf4j_slf4j-api-2.0.7.jar
[2024-06-29T17:05:44.161+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.slf4j_slf4j-api-2.0.7.jar to class loader default
[2024-06-29T17:05:44.161+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.apache.commons_commons-pool2-2.11.1.jar with timestamp 1719680743421
[2024-06-29T17:05:44.162+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.apache.commons_commons-pool2-2.11.1.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp11855305073764130945.tmp
[2024-06-29T17:05:44.163+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp11855305073764130945.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.commons_commons-pool2-2.11.1.jar
[2024-06-29T17:05:44.164+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.commons_commons-pool2-2.11.1.jar to class loader default
[2024-06-29T17:05:44.165+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1719680743421
[2024-06-29T17:05:44.165+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp17813025863028891098.tmp
[2024-06-29T17:05:44.166+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp17813025863028891098.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/com.google.code.findbugs_jsr305-3.0.0.jar
[2024-06-29T17:05:44.167+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default
24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:44.168+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp16515763789884113077.tmp
[2024-06-29T17:05:44.171+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp16515763789884113077.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T17:05:44.172+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.spark_spark-sql-kafka-0-10_2.12-3.5.0.jar to class loader default
[2024-06-29T17:05:44.172+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.lz4_lz4-java-1.8.0.jar with timestamp 1719680743421
[2024-06-29T17:05:44.172+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.lz4_lz4-java-1.8.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp17789125301490718621.tmp
[2024-06-29T17:05:44.174+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp17789125301490718621.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.lz4_lz4-java-1.8.0.jar
[2024-06-29T17:05:44.175+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.lz4_lz4-java-1.8.0.jar to class loader default
[2024-06-29T17:05:44.176+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.postgresql_postgresql-42.5.4.jar with timestamp 1719680743421
[2024-06-29T17:05:44.176+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.postgresql_postgresql-42.5.4.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp4024119371057639847.tmp
[2024-06-29T17:05:44.178+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp4024119371057639847.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.postgresql_postgresql-42.5.4.jar
[2024-06-29T17:05:44.179+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.postgresql_postgresql-42.5.4.jar to class loader default
[2024-06-29T17:05:44.180+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar with timestamp 1719680743421
[2024-06-29T17:05:44.180+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.xerial.snappy_snappy-java-1.1.10.3.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp5075001386528818302.tmp
[2024-06-29T17:05:44.184+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp5075001386528818302.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.xerial.snappy_snappy-java-1.1.10.3.jar
[2024-06-29T17:05:44.185+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.xerial.snappy_snappy-java-1.1.10.3.jar to class loader default
[2024-06-29T17:05:44.185+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1719680743421
[2024-06-29T17:05:44.185+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp17559219331363899245.tmp
[2024-06-29T17:05:44.186+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp17559219331363899245.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/commons-logging_commons-logging-1.1.3.jar
[2024-06-29T17:05:44.187+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/commons-logging_commons-logging-1.1.3.jar to class loader default
24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:44.187+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp18009601598675640074.tmp
[2024-06-29T17:05:44.188+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp18009601598675640074.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar
[2024-06-29T17:05:44.190+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.5.0.jar to class loader default
[2024-06-29T17:05:44.190+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.checkerframework_checker-qual-3.5.0.jar with timestamp 1719680743421
[2024-06-29T17:05:44.190+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.checkerframework_checker-qual-3.5.0.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp1926426481395652554.tmp
[2024-06-29T17:05:44.191+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp1926426481395652554.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.checkerframework_checker-qual-3.5.0.jar
[2024-06-29T17:05:44.192+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.checkerframework_checker-qual-3.5.0.jar to class loader default
[2024-06-29T17:05:44.193+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Fetching spark://localhost:33871/jars/org.apache.kafka_kafka-clients-3.4.1.jar with timestamp 1719680743421
[2024-06-29T17:05:44.193+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Fetching spark://localhost:33871/jars/org.apache.kafka_kafka-clients-3.4.1.jar to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp8987898081874063349.tmp
[2024-06-29T17:05:44.198+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/fetchFileTemp8987898081874063349.tmp has been previously copied to /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.kafka_kafka-clients-3.4.1.jar
[2024-06-29T17:05:44.200+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Executor: Adding file:/tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/userFiles-500828e1-b680-4846-bbee-dccca91e7f8c/org.apache.kafka_kafka-clients-3.4.1.jar to class loader default
[2024-06-29T17:05:44.208+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35041.
[2024-06-29T17:05:44.209+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO NettyBlockTransferService: Server created on localhost:35041
[2024-06-29T17:05:44.209+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2024-06-29T17:05:44.210+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, localhost, 35041, None)
[2024-06-29T17:05:44.213+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO BlockManagerMasterEndpoint: Registering block manager localhost:35041 with 434.4 MiB RAM, BlockManagerId(driver, localhost, 35041, None)
[2024-06-29T17:05:44.214+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, localhost, 35041, None)
[2024-06-29T17:05:44.215+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, localhost, 35041, None)
[2024-06-29T17:05:44.425+0000] {docker.py:413} INFO - 2024-06-29 17:05:44,424:create_spark_session:INFO:Spark session created successfully
[2024-06-29T17:05:44.430+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2024-06-29T17:05:44.431+0000] {docker.py:413} INFO - 24/06/29 17:05:44 INFO SharedState: Warehouse path is 'file:/opt/bitnami/spark/spark-warehouse'.
[2024-06-29T17:05:45.160+0000] {docker.py:413} INFO - 2024-06-29 17:05:45,159:create_initial_dataframe:INFO:Initial dataframe created successfully
[2024-06-29T17:05:45.430+0000] {docker.py:413} INFO - 2024-06-29 17:05:45,429:start_streaming:INFO:Start streaming ...
[2024-06-29T17:05:45.540+0000] {docker.py:413} INFO - 2024-06-29 17:05:45,539:run:INFO:Callback Server Starting
2024-06-29 17:05:45,539:run:INFO:Socket listening on ('127.0.0.1', 41855)
[2024-06-29T17:05:45.551+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
[2024-06-29T17:05:45.565+0000] {docker.py:413} INFO - 24/06/29 17:05:45 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.
[2024-06-29T17:05:45.577+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO ResolveWriteToStream: Checkpoint root file:///tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65 resolved to file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65.
[2024-06-29T17:05:45.577+0000] {docker.py:413} INFO - 24/06/29 17:05:45 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.
[2024-06-29T17:05:45.613+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/metadata using temp file file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/.metadata.93d4e278-f250-46bc-a90b-04df81c79993.tmp
[2024-06-29T17:05:45.653+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/.metadata.93d4e278-f250-46bc-a90b-04df81c79993.tmp to file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/metadata
[2024-06-29T17:05:45.673+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO MicroBatchExecution: Starting [id = 8df91a66-ecb7-4207-a7fd-5824d03cb84d, runId = 96a01030-20af-40bb-bf3a-670d399b989d]. Use file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65 to store the query checkpoint.
[2024-06-29T17:05:45.678+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@1682db68] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@1c3a64d9]
[2024-06-29T17:05:45.694+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-29T17:05:45.695+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO OffsetSeqLog: BatchIds found from listing:
[2024-06-29T17:05:45.696+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO MicroBatchExecution: Starting new streaming query.
[2024-06-29T17:05:45.696+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO MicroBatchExecution: Stream started from {}
[2024-06-29T17:05:45.848+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO AdminClientConfig: AdminClientConfig values: 
	auto.include.jmx.reporter = true
	bootstrap.servers = [kafka:9092]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
[2024-06-29T17:05:45.885+0000] {docker.py:413} INFO - 24/06/29 17:05:45 WARN AdminClientConfig: These configurations '[key.deserializer, value.deserializer, enable.auto.commit, max.poll.records, auto.offset.reset]' were supplied but are not used yet.
[2024-06-29T17:05:45.886+0000] {docker.py:413} INFO - 24/06/29 17:05:45 INFO AppInfoParser: Kafka version: 3.4.1
24/06/29 17:05:45 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
24/06/29 17:05:45 INFO AppInfoParser: Kafka startTimeMs: 1719680745885
[2024-06-29T17:05:46.068+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/sources/0/0 using temp file file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/sources/0/.0.3fd660e7-b3c5-49b1-9936-8e4d23de9004.tmp
[2024-06-29T17:05:46.079+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/sources/0/.0.3fd660e7-b3c5-49b1-9936-8e4d23de9004.tmp to file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/sources/0/0
[2024-06-29T17:05:46.080+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO KafkaMicroBatchStream: Initial offsets: {"current_headlines":{"0":0}}
[2024-06-29T17:05:46.091+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CheckpointFileManager: Writing atomically to file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/offsets/0 using temp file file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/offsets/.0.8e23436f-ed1f-44bc-97f1-25f5777e1dfe.tmp
[2024-06-29T17:05:46.105+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CheckpointFileManager: Renamed temp file file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/offsets/.0.8e23436f-ed1f-44bc-97f1-25f5777e1dfe.tmp to file:/tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65/offsets/0
[2024-06-29T17:05:46.105+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO MicroBatchExecution: Committed offsets for batch 0. Metadata OffsetSeqMetadata(0,1719680746084,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))
[2024-06-29T17:05:46.286+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-29T17:05:46.319+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-29T17:05:46.354+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-29T17:05:46.355+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO KafkaOffsetReaderAdmin: Partitions added: Map()
[2024-06-29T17:05:46.540+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CodeGenerator: Code generated in 102.549292 ms
[2024-06-29T17:05:46.599+0000] {docker.py:413} INFO - 2024-06-29 17:05:46,598:wait_for_commands:INFO:Python Server ready to receive messages
2024-06-29 17:05:46,598:wait_for_commands:INFO:Received command c on object id p0
[2024-06-29T17:05:46.781+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CodeGenerator: Code generated in 34.873375 ms
[2024-06-29T17:05:46.791+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CodeGenerator: Code generated in 9.222292 ms
[2024-06-29T17:05:46.800+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CodeGenerator: Code generated in 6.17525 ms
[2024-06-29T17:05:46.884+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CodeGenerator: Code generated in 5.500625 ms
[2024-06-29T17:05:46.888+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO CodeGenerator: Code generated in 3.740667 ms
[2024-06-29T17:05:46.940+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0
[2024-06-29T17:05:46.947+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO DAGScheduler: Registering RDD 11 (start at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2024-06-29T17:05:46.952+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO DAGScheduler: Registering RDD 7 (start at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2024-06-29T17:05:46.953+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 200 output partitions
[2024-06-29T17:05:46.953+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO DAGScheduler: Final stage: ResultStage 2 (start at NativeMethodAccessorImpl.java:0)
[2024-06-29T17:05:46.953+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0, ShuffleMapStage 1)
[2024-06-29T17:05:46.954+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0, ShuffleMapStage 1)
[2024-06-29T17:05:46.956+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-29T17:05:46.984+0000] {docker.py:413} INFO - 24/06/29 17:05:46 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 14.0 KiB, free 434.4 MiB)
[2024-06-29T17:05:47.002+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.4 KiB, free 434.4 MiB)
[2024-06-29T17:05:47.003+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:35041 (size: 7.4 KiB, free: 434.4 MiB)
[2024-06-29T17:05:47.005+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1580
[2024-06-29T17:05:47.017+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[11] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2024-06-29T17:05:47.018+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2024-06-29T17:05:47.031+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-29T17:05:47.033+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 38.8 KiB, free 434.3 MiB)
[2024-06-29T17:05:47.039+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 434.3 MiB)
[2024-06-29T17:05:47.040+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:35041 (size: 15.2 KiB, free: 434.4 MiB)
[2024-06-29T17:05:47.041+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1580
[2024-06-29T17:05:47.041+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[7] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
24/06/29 17:05:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2024-06-29T17:05:47.050+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (localhost, executor driver, partition 0, PROCESS_LOCAL, 9967 bytes)
[2024-06-29T17:05:47.064+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2024-06-29T17:05:47.065+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (localhost, executor driver, partition 0, PROCESS_LOCAL, 11067 bytes)
[2024-06-29T17:05:47.065+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
[2024-06-29T17:05:47.133+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO CodeGenerator: Code generated in 16.585291 ms
[2024-06-29T17:05:47.145+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO CodeGenerator: Code generated in 10.702126 ms
[2024-06-29T17:05:47.154+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO CodeGenerator: Code generated in 6.727375 ms
[2024-06-29T17:05:47.166+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO CodeGenerator: Code generated in 7.560458 ms
[2024-06-29T17:05:47.168+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO CodeGenerator: Code generated in 3.307375 ms
[2024-06-29T17:05:47.173+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO KafkaBatchReaderFactory: Creating Kafka reader topicPartition=current_headlines-0 fromOffset=0 untilOffset=618, for query queryId=8df91a66-ecb7-4207-a7fd-5824d03cb84d batchId=0 taskId=1 partitionId=0
[2024-06-29T17:05:47.190+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO JDBCRDD: closed connection
[2024-06-29T17:05:47.202+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO CodeGenerator: Code generated in 5.584833 ms
[2024-06-29T17:05:47.223+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO CodeGenerator: Code generated in 15.112083 ms
24/06/29 17:05:47 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2029 bytes result sent to driver
[2024-06-29T17:05:47.230+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 185 ms on localhost (executor driver) (1/1)
[2024-06-29T17:05:47.231+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2024-06-29T17:05:47.241+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO DAGScheduler: ShuffleMapStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 0.280 s
[2024-06-29T17:05:47.242+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO DAGScheduler: looking for newly runnable stages
[2024-06-29T17:05:47.244+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO DAGScheduler: running: Set(ShuffleMapStage 1)
[2024-06-29T17:05:47.244+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2024-06-29T17:05:47.244+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO DAGScheduler: failed: Set()
[2024-06-29T17:05:47.244+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO ConsumerConfig: ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = none
	bootstrap.servers = [kafka:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
[2024-06-29T17:05:47.272+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO AppInfoParser: Kafka version: 3.4.1
[2024-06-29T17:05:47.272+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO AppInfoParser: Kafka commitId: 8a516edc2755df89
[2024-06-29T17:05:47.272+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO AppInfoParser: Kafka startTimeMs: 1719680747271
[2024-06-29T17:05:47.273+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Assigned to partition(s): current_headlines-0
[2024-06-29T17:05:47.278+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Seeking to offset 0 for partition current_headlines-0
[2024-06-29T17:05:47.284+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Resetting the last seen epoch of partition current_headlines-0 to 6 since the associated topicId changed from null to oTQJXETvSIybCiV1hWStYA
[2024-06-29T17:05:47.285+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Cluster ID: G13gPvLWR7u_hUqWhAPD3A
[2024-06-29T17:05:47.374+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Seeking to earliest offset of partition current_headlines-0
[2024-06-29T17:05:47.378+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Resetting offset for partition current_headlines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=6}}.
[2024-06-29T17:05:47.379+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Seeking to latest offset of partition current_headlines-0
[2024-06-29T17:05:47.379+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Resetting offset for partition current_headlines-0 to position FetchPosition{offset=618, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=6}}.
[2024-06-29T17:05:47.509+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Seeking to offset 500 for partition current_headlines-0
[2024-06-29T17:05:47.510+0000] {docker.py:413} INFO - 24/06/29 17:05:47 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Seeking to earliest offset of partition current_headlines-0
[2024-06-29T17:05:48.013+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Resetting offset for partition current_headlines-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=6}}.
[2024-06-29T17:05:48.015+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Seeking to latest offset of partition current_headlines-0
[2024-06-29T17:05:48.015+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Resetting offset for partition current_headlines-0 to position FetchPosition{offset=618, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[kafka:9092 (id: 0 rack: null)], epoch=6}}.
[2024-06-29T17:05:48.056+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO KafkaDataConsumer: From Kafka topicPartition=current_headlines-0 groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor read 618 records through 2 polls (polled  out 618 records), taking 607737125 nanos, during time span of 780107459 nanos.
[2024-06-29T17:05:48.062+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2444 bytes result sent to driver
[2024-06-29T17:05:48.063+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1001 ms on localhost (executor driver) (1/1)
24/06/29 17:05:48 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2024-06-29T17:05:48.064+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO DAGScheduler: ShuffleMapStage 1 (start at NativeMethodAccessorImpl.java:0) finished in 1.032 s
24/06/29 17:05:48 INFO DAGScheduler: looking for newly runnable stages
24/06/29 17:05:48 INFO DAGScheduler: running: Set()
24/06/29 17:05:48 INFO DAGScheduler: waiting: Set(ResultStage 2)
[2024-06-29T17:05:48.065+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO DAGScheduler: failed: Set()
[2024-06-29T17:05:48.065+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[18] at start at NativeMethodAccessorImpl.java:0), which has no missing parents
[2024-06-29T17:05:48.094+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:35041 in memory (size: 7.4 KiB, free: 434.4 MiB)
[2024-06-29T17:05:48.100+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 72.4 KiB, free 434.3 MiB)
[2024-06-29T17:05:48.101+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 31.7 KiB, free 434.2 MiB)
[2024-06-29T17:05:48.102+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:35041 (size: 31.7 KiB, free: 434.4 MiB)
[2024-06-29T17:05:48.102+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1580
[2024-06-29T17:05:48.104+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO DAGScheduler: Submitting 200 missing tasks from ResultStage 2 (MapPartitionsRDD[18] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2024-06-29T17:05:48.104+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSchedulerImpl: Adding task set 2.0 with 200 tasks resource profile 0
[2024-06-29T17:05:48.110+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 2) (localhost, executor driver, partition 1, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.111+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID 3) (localhost, executor driver, partition 2, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.111+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 3.0 in stage 2.0 (TID 4) (localhost, executor driver, partition 3, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.112+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 5.0 in stage 2.0 (TID 5) (localhost, executor driver, partition 5, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.112+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 6.0 in stage 2.0 (TID 6) (localhost, executor driver, partition 6, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.113+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 11.0 in stage 2.0 (TID 7) (localhost, executor driver, partition 11, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.113+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 12.0 in stage 2.0 (TID 8) (localhost, executor driver, partition 12, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.115+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 14.0 in stage 2.0 (TID 9) (localhost, executor driver, partition 14, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.116+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 15.0 in stage 2.0 (TID 10) (localhost, executor driver, partition 15, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.119+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 16.0 in stage 2.0 (TID 11) (localhost, executor driver, partition 16, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.124+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 1.0 in stage 2.0 (TID 2)
24/06/29 17:05:48 INFO Executor: Running task 2.0 in stage 2.0 (TID 3)
[2024-06-29T17:05:48.125+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 3.0 in stage 2.0 (TID 4)
[2024-06-29T17:05:48.126+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 6.0 in stage 2.0 (TID 6)
24/06/29 17:05:48 INFO Executor: Running task 5.0 in stage 2.0 (TID 5)
[2024-06-29T17:05:48.130+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 11.0 in stage 2.0 (TID 7)
[2024-06-29T17:05:48.135+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 12.0 in stage 2.0 (TID 8)
[2024-06-29T17:05:48.136+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 14.0 in stage 2.0 (TID 9)
[2024-06-29T17:05:48.144+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 15.0 in stage 2.0 (TID 10)
[2024-06-29T17:05:48.155+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 16.0 in stage 2.0 (TID 11)
[2024-06-29T17:05:48.227+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1271.0 B) non-empty blocks including 1 (1271.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.229+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.230+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.231+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1051.0 B) non-empty blocks including 1 (1051.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.231+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2024-06-29T17:05:48.231+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2024-06-29T17:05:48.232+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 11 ms
[2024-06-29T17:05:48.237+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO CodeGenerator: Code generated in 5.846416 ms
[2024-06-29T17:05:48.251+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO CodeGenerator: Code generated in 8.143 ms
[2024-06-29T17:05:48.260+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO CodeGenerator: Code generated in 4.517167 ms
[2024-06-29T17:05:48.269+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.270+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.270+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.271+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.271+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.271+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.272+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 2 ms
[2024-06-29T17:05:48.275+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO CodeGenerator: Code generated in 5.623167 ms
[2024-06-29T17:05:48.287+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO CodeGenerator: Code generated in 7.9855 ms
[2024-06-29T17:05:48.555+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:35041 in memory (size: 15.2 KiB, free: 434.4 MiB)
[2024-06-29T17:05:48.877+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO CodeGenerator: Code generated in 6.168875 ms
[2024-06-29T17:05:48.899+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 6.0 in stage 2.0 (TID 6). 6218 bytes result sent to driver
[2024-06-29T17:05:48.900+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 3.0 in stage 2.0 (TID 4). 6218 bytes result sent to driver
[2024-06-29T17:05:48.901+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 2.0 in stage 2.0 (TID 3). 6261 bytes result sent to driver
24/06/29 17:05:48 INFO Executor: Finished task 1.0 in stage 2.0 (TID 2). 6218 bytes result sent to driver
[2024-06-29T17:05:48.903+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 17.0 in stage 2.0 (TID 12) (localhost, executor driver, partition 17, NODE_LOCAL, 10414 bytes) 
24/06/29 17:05:48 INFO Executor: Finished task 15.0 in stage 2.0 (TID 10). 6218 bytes result sent to driver
24/06/29 17:05:48 INFO Executor: Finished task 16.0 in stage 2.0 (TID 11). 6218 bytes result sent to driver
[2024-06-29T17:05:48.903+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 5.0 in stage 2.0 (TID 5). 6261 bytes result sent to driver
24/06/29 17:05:48 INFO Executor: Running task 17.0 in stage 2.0 (TID 12)
[2024-06-29T17:05:48.904+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 19.0 in stage 2.0 (TID 13) (localhost, executor driver, partition 19, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.905+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 19.0 in stage 2.0 (TID 13)
[2024-06-29T17:05:48.908+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 22.0 in stage 2.0 (TID 14) (localhost, executor driver, partition 22, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.908+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 22.0 in stage 2.0 (TID 14)
[2024-06-29T17:05:48.909+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 25.0 in stage 2.0 (TID 15) (localhost, executor driver, partition 25, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.909+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 25.0 in stage 2.0 (TID 15)
[2024-06-29T17:05:48.910+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 26.0 in stage 2.0 (TID 16) (localhost, executor driver, partition 26, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.910+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 26.0 in stage 2.0 (TID 16)
[2024-06-29T17:05:48.910+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1156.0 B) non-empty blocks including 1 (1156.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.912+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO TaskSetManager: Starting task 27.0 in stage 2.0 (TID 17) (localhost, executor driver, partition 27, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.912+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 2) in 798 ms on localhost (executor driver) (1/200)
24/06/29 17:05:48 INFO Executor: Running task 27.0 in stage 2.0 (TID 17)
24/06/29 17:05:48 INFO TaskSetManager: Finished task 6.0 in stage 2.0 (TID 6) in 795 ms on localhost (executor driver) (2/200)
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.913+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 16.0 in stage 2.0 (TID 11) in 795 ms on localhost (executor driver) (3/200)
[2024-06-29T17:05:48.913+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 2.0 in stage 2.0 (TID 3) in 798 ms on localhost (executor driver) (4/200)
[2024-06-29T17:05:48.914+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 30.0 in stage 2.0 (TID 18) (localhost, executor driver, partition 30, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.915+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 30.0 in stage 2.0 (TID 18)
[2024-06-29T17:05:48.918+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 3.0 in stage 2.0 (TID 4) in 799 ms on localhost (executor driver) (5/200)
[2024-06-29T17:05:48.919+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 5.0 in stage 2.0 (TID 5) in 807 ms on localhost (executor driver) (6/200)
[2024-06-29T17:05:48.919+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 15.0 in stage 2.0 (TID 10) in 805 ms on localhost (executor driver) (7/200)
[2024-06-29T17:05:48.921+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (334.0 B) non-empty blocks including 1 (334.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.925+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.925+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (445.0 B) non-empty blocks including 1 (445.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1271.0 B) non-empty blocks including 1 (1271.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.926+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.927+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1399.0 B) non-empty blocks including 1 (1399.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.927+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.928+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.928+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.928+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1692.0 B) non-empty blocks including 1 (1692.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.930+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.931+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.931+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (955.0 B) non-empty blocks including 1 (955.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (97.0 B) non-empty blocks including 1 (97.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.939+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 26.0 in stage 2.0 (TID 16). 6261 bytes result sent to driver
[2024-06-29T17:05:48.941+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 27.0 in stage 2.0 (TID 17). 6218 bytes result sent to driver
24/06/29 17:05:48 INFO TaskSetManager: Starting task 32.0 in stage 2.0 (TID 19) (localhost, executor driver, partition 32, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.942+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 35.0 in stage 2.0 (TID 20) (localhost, executor driver, partition 35, NODE_LOCAL, 10414 bytes) 
24/06/29 17:05:48 INFO Executor: Running task 32.0 in stage 2.0 (TID 19)
[2024-06-29T17:05:48.943+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 26.0 in stage 2.0 (TID 16) in 35 ms on localhost (executor driver) (8/200)
[2024-06-29T17:05:48.944+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 35.0 in stage 2.0 (TID 20)
[2024-06-29T17:05:48.945+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 27.0 in stage 2.0 (TID 17) in 34 ms on localhost (executor driver) (9/200)
[2024-06-29T17:05:48.946+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (868.0 B) non-empty blocks including 1 (868.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (955.0 B) non-empty blocks including 1 (955.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.947+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.947+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.947+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.951+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.955+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 35.0 in stage 2.0 (TID 20). 6175 bytes result sent to driver
[2024-06-29T17:05:48.957+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 36.0 in stage 2.0 (TID 21) (localhost, executor driver, partition 36, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.961+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 36.0 in stage 2.0 (TID 21)
[2024-06-29T17:05:48.962+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 32.0 in stage 2.0 (TID 19). 6175 bytes result sent to driver
[2024-06-29T17:05:48.963+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 35.0 in stage 2.0 (TID 20) in 19 ms on localhost (executor driver) (10/200)
[2024-06-29T17:05:48.964+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 39.0 in stage 2.0 (TID 22) (localhost, executor driver, partition 39, NODE_LOCAL, 10414 bytes) 
24/06/29 17:05:48 INFO TaskSetManager: Finished task 32.0 in stage 2.0 (TID 19) in 20 ms on localhost (executor driver) (11/200)
[2024-06-29T17:05:48.967+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 39.0 in stage 2.0 (TID 22)
[2024-06-29T17:05:48.967+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1538.0 B) non-empty blocks including 1 (1538.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.968+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (106.0 B) non-empty blocks including 1 (106.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.969+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.969+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (88.0 B) non-empty blocks including 1 (88.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.983+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 36.0 in stage 2.0 (TID 21). 6175 bytes result sent to driver
[2024-06-29T17:05:48.990+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 40.0 in stage 2.0 (TID 23) (localhost, executor driver, partition 40, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.992+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 40.0 in stage 2.0 (TID 23)
[2024-06-29T17:05:48.993+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 36.0 in stage 2.0 (TID 21) in 28 ms on localhost (executor driver) (12/200)
[2024-06-29T17:05:48.994+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Finished task 39.0 in stage 2.0 (TID 22). 6175 bytes result sent to driver
[2024-06-29T17:05:48.994+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Starting task 41.0 in stage 2.0 (TID 24) (localhost, executor driver, partition 41, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:48.995+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO Executor: Running task 41.0 in stage 2.0 (TID 24)
[2024-06-29T17:05:48.996+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO TaskSetManager: Finished task 39.0 in stage 2.0 (TID 22) in 30 ms on localhost (executor driver) (13/200)
[2024-06-29T17:05:48.996+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (955.0 B) non-empty blocks including 1 (955.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.996+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.997+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 1 (1051.0 B) non-empty blocks including 1 (1051.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.997+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.997+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:48.998+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2024-06-29T17:05:48.998+0000] {docker.py:413} INFO - 24/06/29 17:05:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:49.029+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR Executor: Exception in task 22.0 in stage 2.0 (TID 14)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:36:40Z','{"id":"politico","name":"Politico"}','POLITICO','Trump debate playbook: Skip mock debates, fundraise and undermine the results - POLITICO',NULL,'https://www.politico.com/news/2024/06/26/trump-debate-preparation-00165187',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:36:40Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:36:40Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
24/06/29 17:05:49 ERROR Executor: Exception in task 14.0 in stage 2.0 (TID 9)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T04:40:45Z','{"id":"cbs-news","name":"CBS News"}','Jesse Sarles, Austen Erblat','Lauren Boebert wins crowded House GOP primary in new Colorado district - CBS News','Republican Rep. Lauren Boebert declared victory on Tuesday night in the primary election in her new Colorado district.','https://www.cbsnews.com/colorado/news/lauren-boebert-wins-colorado-4th-congressional-district-republican-primary/','https://assets2.cbsnewsstatic.com/hub/i/r/2024/06/26/7e070f25-2f00-46f7-b7c8-670413a2d960/thumbnail/1200x630/422748044f79ba0d00e98d2723e5f785/boebert.jpg?v=d44ea471ad55b1f821a0763c85064960','Republican Rep. Lauren Boebert has emerged victorious in the primary election in her new Colorado district.
The controversial Congresswoman defeated five GOP opponents in a competitive primary in Co? [+5329 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T04:40:45Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T04:40:45Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.034+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR Executor: Exception in task 12.0 in stage 2.0 (TID 8)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T17:00:00Z','{"id":"the-verge","name":"The Verge"}','Jay Peters','An AI version of Al Michaels will deliver Olympic recaps on Peacock - The Verge','Peacock?s Paris Olympics coverage will include personalized daily recaps narrated by an AI-generated version of Al Michaels? voice.','https://www.theverge.com/2024/6/26/24185774/olympics-ai-al-michaels-voice-recaps','https://cdn.vox-cdn.com/thumbor/loA3E-uE5D5ufnZcSYeCltrB99o=/0x0:8640x5760/1200x628/filters:focal(5224x2847:5225x2848)/cdn.vox-cdn.com/uploads/chorus_asset/file/25505642/1772478978.jpg','An AI version of Al Michaels will deliver Olympic recaps on Peacock
An AI version of Al Michaels will deliver Olympic recaps on Peacock
 / NBCs AI-generated voice could create nearly 7 million cust? [+2697 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T17:00:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T17:00:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
24/06/29 17:05:49 ERROR Executor: Exception in task 17.0 in stage 2.0 (TID 12)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T05:46:56Z','{"id":null,"name":"NPR"}',NULL,'FDA warns top U.S. bakery not to claim foods contain allergens when they don''t - NPR','The FDA found Bimbo Bakeries USA ? which includes brands such as Sara Lee and Ball Park buns and rolls ? listed ingredients such as sesame or tree nuts on labels even when they weren''t in the foods.','https://www.npr.org/2024/06/26/g-s1-6238/fda-warns-bakery-foods-allergens','https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2000x1125+0+104/resize/1400/quality/100/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F16%2Fe55fa0fb497f9e722a56bbbc2b8f%2Fap24177785376325.jpg','Federal food safety regulators said Tuesday that they have warned a top U.S. bakery to stop using labels that say its products contain potentially dangerous allergens when they don''t.
U.S. Food and ? [+2568 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T05:46:56Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T05:46:56Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.035+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR Executor: Exception in task 11.0 in stage 2.0 (TID 7)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T21:06:07Z','{"id":"al-jazeera-english","name":"Al Jazeera English"}','Al Jazeera','Troops begin to withdraw as Bolivian president rallies against coup attempt - Al Jazeera English','Regional organisations mobilised behind Bolivian government after troops and armoured vehicles gathered in the capital.','https://www.aljazeera.com/news/2024/6/26/fears-of-coup-attempt-in-bolivia-as-soldiers-storm-presidential-palace','https://www.aljazeera.com/wp-content/uploads/2024/06/AP24178775725792-1719438488.jpg?resize=1200%2C675','Bolivian President Luis Arce has offered his thanks to the people of the country after facing down a coup attempt that drew international condemnation and saw soldiers smash through the doors of the ? [+4381 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T21:06:07Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T21:06:07Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.035+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR Executor: Exception in task 30.0 in stage 2.0 (TID 18)
java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.036+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR Executor: Exception in task 19.0 in stage 2.0 (TID 13)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.037+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR Executor: Exception in task 25.0 in stage 2.0 (TID 15)
java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T19:20:00Z','{"id":"cnn","name":"CNN"}','Taylor Nicioli','Decades after the famed Kyrenia shipwreck?s discovery, researchers have a new estimate of when it sank - CNN','The timeline of a Hellenistic Kyrenia shipwreck stumped researchers for decades. But thanks to a cache of ancient almonds, a new study may have a better estimate.','https://www.cnn.com/2024/06/26/science/kyrenia-shipwreck-radiocarbon-dating-almonds-scn/index.html','https://media.cnn.com/api/v1/images/stellar/prod/kyrenia-ship-hull-during-excavation.jpg?c=16x9&q=w_800,c_fill','Sign up for CNNs Wonder Theory science newsletter.?Explore the universe with news on fascinating discoveries, scientific advancements and more.
A lone diverfirst laid eyes on the ancient Kyrenia shi? [+6426 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T19:20:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T19:20:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.039+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO TaskSetManager: Starting task 42.0 in stage 2.0 (TID 25) (localhost, executor driver, partition 42, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:49.041+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO Executor: Running task 42.0 in stage 2.0 (TID 25)
[2024-06-29T17:05:49.042+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO TaskSetManager: Starting task 44.0 in stage 2.0 (TID 26) (localhost, executor driver, partition 44, NODE_LOCAL, 10414 bytes)
[2024-06-29T17:05:49.042+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO Executor: Running task 44.0 in stage 2.0 (TID 26)
[2024-06-29T17:05:49.042+0000] {docker.py:413} INFO - 24/06/29 17:05:49 WARN TaskSetManager: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.042+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR Executor: Exception in task 41.0 in stage 2.0 (TID 24)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:44:00Z','{"id":"associated-press","name":"Associated Press"}','LARRY NEUMEISTER','Former Honduran President sentenced to 45 years in US prison - The Associated Press','Former Honduran President Juan Orlando Hern?ndez has been sentenced to 45 years in prison and fined $8 million for enabling drug traffickers to use his military and national police force to help get tons of cocaine into the United States. The 55-year-old was ?','https://apnews.com/article/honduras-president-juan-orlando-hernandez-corruption-3f98be974c58bb8a1b492108c4a7f297','https://dims.apnews.com/dims4/default/4c48030/2147483647/strip/true/crop/5760x3240+0+300/resize/1440x810!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F87%2Fd8%2Fe5c7def9831299395779f42be2f6%2F4d1cc18e0f644c768cb5e46583f3bc55','NEW YORK (AP) A defiant former Honduran President Juan Orlando Hern?ndez was sentenced in New York Wednesday to 45 years in prison for teaming up with some bribe-paying drug traffickers for over a de? [+5370 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:44:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:44:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.043+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR Executor: Exception in task 40.0 in stage 2.0 (TID 23)
java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T14:14:28Z','{"id":null,"name":"BBC News"}',NULL,'Mishal Husain: How I?ll referee BBC leaders'' debate with voters at its heart - BBC.com','Ahead of the BBC debate, the Today programme presenter explains how she will ensure questions are answered.','https://www.bbc.com/news/articles/c6pp2kyg2k6o','https://ichef.bbci.co.uk/news/1024/branded_news/7794/live/71cb0130-3304-11ef-9f4c-8785b0c18838.png','On Wednesday night, on a debate stage built in an atrium at Nottingham Trent University, the two men who want to lead the country will face each other directly for the last time in this election camp? [+2359 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T14:14:28Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T14:14:28Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.043+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR TaskSetManager: Task 30 in stage 2.0 failed 1 times; aborting job
[2024-06-29T17:05:49.044+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO TaskSchedulerImpl: Cancelling stage 2
[2024-06-29T17:05:49.045+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage cancelled: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-06-29T17:05:49.051+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO Executor: Executor is trying to kill task 42.0 in stage 2.0 (TID 25), reason: Stage cancelled: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 17:05:49 INFO Executor: Executor is trying to kill task 44.0 in stage 2.0 (TID 26), reason: Stage cancelled: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-06-29T17:05:49.055+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO ShuffleBlockFetcherIterator: Getting 1 (789.0 B) non-empty blocks including 1 (789.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
[2024-06-29T17:05:49.055+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO TaskSchedulerImpl: Stage 2 was cancelled
[2024-06-29T17:05:49.059+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks, null
24/06/29 17:05:49 INFO DAGScheduler: ResultStage 2 (start at NativeMethodAccessorImpl.java:0) failed in 0.961 s due to Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 17:05:49 WARN TaskSetManager: Lost task 19.0 in stage 2.0 (TID 13) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T16:00:13Z','{"id":null,"name":"Space.com"}','Andrew Jones','What happened to China''s Chang''e 6 lander on the moon''s far side? - Space.com','The Chang''e 6 lander wasn''t equipped with heaters to help it survive the long, cold lunar night.','https://www.space.com/china-chang-e-6-moon-lander-far-side-fate','https://cdn.mos.cms.futurecdn.net/LEb2kLUkdpZay9QTj8srWS-1200-80.png','China''s Chang''e 6 mission has successfully delivered to Earth the first-ever samples from the far side of the moon. But what became of the lander that collected the lunar material?
Chang''e 6 launche? [+3742 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T16:00:13Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

24/06/29 17:05:49 INFO ShuffleBlockFetcherIterator: Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
24/06/29 17:05:49 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
[2024-06-29T17:05:49.060+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR ShuffleBlockFetcherIterator: Error occurred while fetching local blocks, null
24/06/29 17:05:49 WARN TaskSetManager: Lost task 12.0 in stage 2.0 (TID 8) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T17:00:00Z','{"id":"the-verge","name":"The Verge"}','Jay Peters','An AI version of Al Michaels will deliver Olympic recaps on Peacock - The Verge','Peacock?s Paris Olympics coverage will include personalized daily recaps narrated by an AI-generated version of Al Michaels? voice.','https://www.theverge.com/2024/6/26/24185774/olympics-ai-al-michaels-voice-recaps','https://cdn.vox-cdn.com/thumbor/loA3E-uE5D5ufnZcSYeCltrB99o=/0x0:8640x5760/1200x628/filters:focal(5224x2847:5225x2848)/cdn.vox-cdn.com/uploads/chorus_asset/file/25505642/1772478978.jpg','An AI version of Al Michaels will deliver Olympic recaps on Peacock
An AI version of Al Michaels will deliver Olympic recaps on Peacock
 / NBCs AI-generated voice could create nearly 7 million cust? [+2697 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T17:00:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T17:00:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

24/06/29 17:05:49 INFO Executor: Executor interrupted and killed task 42.0 in stage 2.0 (TID 25), reason: Stage cancelled: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
24/06/29 17:05:49 INFO Executor: Executor interrupted and killed task 44.0 in stage 2.0 (TID 26), reason: Stage cancelled: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
[2024-06-29T17:05:49.061+0000] {docker.py:413} INFO - 24/06/29 17:05:49 WARN TaskSetManager: Lost task 25.0 in stage 2.0 (TID 15) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T19:20:00Z','{"id":"cnn","name":"CNN"}','Taylor Nicioli','Decades after the famed Kyrenia shipwreck?s discovery, researchers have a new estimate of when it sank - CNN','The timeline of a Hellenistic Kyrenia shipwreck stumped researchers for decades. But thanks to a cache of ancient almonds, a new study may have a better estimate.','https://www.cnn.com/2024/06/26/science/kyrenia-shipwreck-radiocarbon-dating-almonds-scn/index.html','https://media.cnn.com/api/v1/images/stellar/prod/kyrenia-ship-hull-during-excavation.jpg?c=16x9&q=w_800,c_fill','Sign up for CNNs Wonder Theory science newsletter.?Explore the universe with news on fascinating discoveries, scientific advancements and more.
A lone diverfirst laid eyes on the ancient Kyrenia shi? [+6426 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T19:20:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T19:20:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.062+0000] {docker.py:413} INFO - 24/06/29 17:05:49 WARN TaskSetManager: Lost task 22.0 in stage 2.0 (TID 14) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:36:40Z','{"id":"politico","name":"Politico"}','POLITICO','Trump debate playbook: Skip mock debates, fundraise and undermine the results - POLITICO',NULL,'https://www.politico.com/news/2024/06/26/trump-debate-preparation-00165187',NULL,NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:36:40Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:36:40Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

24/06/29 17:05:49 WARN TaskSetManager: Lost task 41.0 in stage 2.0 (TID 24) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:44:00Z','{"id":"associated-press","name":"Associated Press"}','LARRY NEUMEISTER','Former Honduran President sentenced to 45 years in US prison - The Associated Press','Former Honduran President Juan Orlando Hern?ndez has been sentenced to 45 years in prison and fined $8 million for enabling drug traffickers to use his military and national police force to help get tons of cocaine into the United States. The 55-year-old was ?','https://apnews.com/article/honduras-president-juan-orlando-hernandez-corruption-3f98be974c58bb8a1b492108c4a7f297','https://dims.apnews.com/dims4/default/4c48030/2147483647/strip/true/crop/5760x3240+0+300/resize/1440x810!/quality/90/?url=https%3A%2F%2Fassets.apnews.com%2F87%2Fd8%2Fe5c7def9831299395779f42be2f6%2F4d1cc18e0f644c768cb5e46583f3bc55','NEW YORK (AP) A defiant former Honduran President Juan Orlando Hern?ndez was sentenced in New York Wednesday to 45 years in prison for teaming up with some bribe-paying drug traffickers for over a de? [+5370 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:44:00Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:44:00Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.062+0000] {docker.py:413} INFO - 24/06/29 17:05:49 WARN TaskSetManager: Lost task 40.0 in stage 2.0 (TID 23) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T14:14:28Z','{"id":null,"name":"BBC News"}',NULL,'Mishal Husain: How I?ll referee BBC leaders'' debate with voters at its heart - BBC.com','Ahead of the BBC debate, the Today programme presenter explains how she will ensure questions are answered.','https://www.bbc.com/news/articles/c6pp2kyg2k6o','https://ichef.bbci.co.uk/news/1024/branded_news/7794/live/71cb0130-3304-11ef-9f4c-8785b0c18838.png','On Wednesday night, on a debate stage built in an atrium at Nottingham Trent University, the two men who want to lead the country will face each other directly for the last time in this election camp? [+2359 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T14:14:28Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T14:14:28Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.063+0000] {docker.py:413} INFO - 24/06/29 17:05:49 WARN TaskSetManager: Lost task 11.0 in stage 2.0 (TID 7) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T21:06:07Z','{"id":"al-jazeera-english","name":"Al Jazeera English"}','Al Jazeera','Troops begin to withdraw as Bolivian president rallies against coup attempt - Al Jazeera English','Regional organisations mobilised behind Bolivian government after troops and armoured vehicles gathered in the capital.','https://www.aljazeera.com/news/2024/6/26/fears-of-coup-attempt-in-bolivia-as-soldiers-storm-presidential-palace','https://www.aljazeera.com/wp-content/uploads/2024/06/AP24178775725792-1719438488.jpg?resize=1200%2C675','Bolivian President Luis Arce has offered his thanks to the people of the country after facing down a coup attempt that drew international condemnation and saw soldiers smash through the doors of the ? [+4381 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T21:06:07Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T21:06:07Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.063+0000] {docker.py:413} INFO - 24/06/29 17:05:49 WARN TaskSetManager: Lost task 14.0 in stage 2.0 (TID 9) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T04:40:45Z','{"id":"cbs-news","name":"CBS News"}','Jesse Sarles, Austen Erblat','Lauren Boebert wins crowded House GOP primary in new Colorado district - CBS News','Republican Rep. Lauren Boebert declared victory on Tuesday night in the primary election in her new Colorado district.','https://www.cbsnews.com/colorado/news/lauren-boebert-wins-colorado-4th-congressional-district-republican-primary/','https://assets2.cbsnewsstatic.com/hub/i/r/2024/06/26/7e070f25-2f00-46f7-b7c8-670413a2d960/thumbnail/1200x630/422748044f79ba0d00e98d2723e5f785/boebert.jpg?v=d44ea471ad55b1f821a0763c85064960','Republican Rep. Lauren Boebert has emerged victorious in the primary election in her new Colorado district.
The controversial Congresswoman defeated five GOP opponents in a competitive primary in Co? [+5329 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T04:40:45Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T04:40:45Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.063+0000] {docker.py:413} INFO - 24/06/29 17:05:49 WARN TaskSetManager: Lost task 17.0 in stage 2.0 (TID 12) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 1 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T05:46:56Z','{"id":null,"name":"NPR"}',NULL,'FDA warns top U.S. bakery not to claim foods contain allergens when they don''t - NPR','The FDA found Bimbo Bakeries USA ? which includes brands such as Sara Lee and Ball Park buns and rolls ? listed ingredients such as sesame or tree nuts on labels even when they weren''t in the foods.','https://www.npr.org/2024/06/26/g-s1-6238/fda-warns-bakery-foods-allergens','https://npr.brightspotcdn.com/dims3/default/strip/false/crop/2000x1125+0+104/resize/1400/quality/100/format/jpeg/?url=http%3A%2F%2Fnpr-brightspot.s3.amazonaws.com%2Fd8%2F16%2Fe55fa0fb497f9e722a56bbbc2b8f%2Fap24177785376325.jpg','Federal food safety regulators said Tuesday that they have warned a top U.S. bakery to stop using labels that say its products contain potentially dangerous allergens when they don''t.
U.S. Food and ? [+2568 chars]') was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T05:46:56Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T05:46:56Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

24/06/29 17:05:49 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-06-29T17:05:49.064+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO DAGScheduler: Job 0 failed: start at NativeMethodAccessorImpl.java:0, took 2.117110 s
[2024-06-29T17:05:49.064+0000] {docker.py:413} INFO - 24/06/29 17:05:49 WARN TaskSetManager: Lost task 42.0 in stage 2.0 (TID 25) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
24/06/29 17:05:49 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
24/06/29 17:05:49 WARN TaskSetManager: Lost task 44.0 in stage 2.0 (TID 26) (localhost executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:)
24/06/29 17:05:49 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2024-06-29T17:05:49.198+0000] {docker.py:413} INFO - 2024-06-29 17:05:49,152:_call_proxy:ERROR:There was an exception while executing the Python Proxy on the Python Side.
Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 83, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos…','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos…','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.273+0000] {docker.py:413} INFO - 24/06/29 17:05:49 ERROR MicroBatchExecution: Query [id = 8df91a66-ecb7-4207-a7fd-5824d03cb84d, runId = 96a01030-20af-40bb-bf3a-670d399b989d] terminated with error
py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 83, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos?','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more


	at py4j.Protocol.getReturnValue(Protocol.java:476)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
[2024-06-29T17:05:49.275+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO AppInfoParser: App info kafka.admin.client for adminclient-1 unregistered
[2024-06-29T17:05:49.276+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO Metrics: Metrics scheduler closed
24/06/29 17:05:49 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-29T17:05:49.277+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO Metrics: Metrics reporters closed
[2024-06-29T17:05:49.277+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO MicroBatchExecution: Async log purge executor pool for query [id = 8df91a66-ecb7-4207-a7fd-5824d03cb84d, runId = 96a01030-20af-40bb-bf3a-670d399b989d] has been shutdown
[2024-06-29T17:05:49.484+0000] {docker.py:413} INFO - Traceback (most recent call last):
  File "/opt/bitnami/spark/spark_streaming.py", line 103, in <module>
[2024-06-29T17:05:49.485+0000] {docker.py:413} INFO - write_to_postgres()
  File "/opt/bitnami/spark/spark_streaming.py", line 99, in write_to_postgres
    start_streaming(df_final, spark=spark)
  File "/opt/bitnami/spark/spark_streaming.py", line 90, in start_streaming
    return query.awaitTermination()
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/streaming/query.py", line 221, in awaitTermination
[2024-06-29T17:05:49.485+0000] {docker.py:413} INFO - File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2024-06-29T17:05:49.485+0000] {docker.py:413} INFO - File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2024-06-29T17:05:49.486+0000] {docker.py:413} INFO - pyspark.errors.exceptions.captured.StreamingQueryException
[2024-06-29T17:05:49.491+0000] {docker.py:413} INFO - : [STREAM_FAILED] Query [id = 8df91a66-ecb7-4207-a7fd-5824d03cb84d, runId = 96a01030-20af-40bb-bf3a-670d399b989d] terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py", line 617, in _call_proxy
    return_value = getattr(self.pool[obj_id], method)(*params)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 120, in call
    raise e
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 117, in call
    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)
  File "/opt/bitnami/spark/spark_streaming.py", line 83, in <lambda>
    .write.jdbc(
           ^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
    self.mode(mode)._jwrite.jdbc(url, table, jprop)
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
    return_value = get_return_value(
                   ^^^^^^^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
    return f(*a, **kw)
           ^^^^^^^^^^^
  File "/opt/bitnami/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
    raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred while calling o69.jdbc.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 30 in stage 2.0 failed 1 times, most recent failure: Lost task 30.0 in stage 2.0 (TID 18) (localhost executor driver): java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos…','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)
	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)
	at scala.Option.foreach(Option.scala:407)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2463)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1036)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:407)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1034)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:901)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:70)
	at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)
	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)
	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:248)
	at org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:756)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:568)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:384)
	at py4j.CallbackClient.sendCommand(CallbackClient.java:356)
	at py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)
	at jdk.proxy3/jdk.proxy3.$Proxy30.call(Unknown Source)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)
	at org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)
	at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)
	at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)
	at org.apache.spark.sql.execution.streaming.SingleBatchExecutor.execute(TriggerExecutor.scala:39)
	at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)
	at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
	at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)
	at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)
Caused by: java.sql.BatchUpdateException: Batch entry 2 INSERT INTO current_headlines_table ("publishedat","source","author","title","description","url","urltoimage","content") VALUES ('2024-06-26T22:00:35Z','{"id":null,"name":"The Weather Channel"}','The Weather Channel','Hundreds Of Ice Cream, Coffee Products Recalled - Videos from The Weather Channel - The Weather Channel','The FDA has announced two widespread recalls of coffee and ice cream products. Find out if your favorites are on the list. <a href="https://www.fda.gov/safety/recalls-market-withdrawals-safety-alerts/totally-cool-inc-recalls-all-ice-cream-products-because-pos…','https://weather.com/home-garden/food/video/summer-treat-recalls-is-your-favorite-on-the-list','https://v.w-x.co/1719439224584_0626_fda_recall_v1_35199b66-0605-4711-a2af-f50a29aef4f2.jpg',NULL) was aborted: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.  Call getNextException to see other errors in the batch.
	at org.postgresql.jdbc.BatchResultHandler.handleError(BatchResultHandler.java:165)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2367)
	at org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:560)
	at org.postgresql.jdbc.PgStatement.internalExecuteBatch(PgStatement.java:893)
	at org.postgresql.jdbc.PgStatement.executeBatch(PgStatement.java:916)
	at org.postgresql.jdbc.PgPreparedStatement.executeBatch(PgPreparedStatement.java:1684)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:751)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:902)
	at org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:901)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1036)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1036)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2438)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
	at org.apache.spark.scheduler.Task.run(Task.scala:141)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
	at java.base/java.lang.Thread.run(Thread.java:840)
Caused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint "current_headlines_table_pkey"
  Detail: Key (publishedat)=(2024-06-26T22:00:35Z) already exists.
	at org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2676)
	at org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2366)
	... 21 more
[2024-06-29T17:05:49.492+0000] {docker.py:413} INFO - 2024-06-29 17:05:49,488:close:INFO:Closing down clientserver connection
[2024-06-29T17:05:49.511+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Resetting generation and member id due to: consumer pro-actively leaving the group
24/06/29 17:05:49 INFO ConsumerCoordinator: [Consumer clientId=consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1, groupId=spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor] Request joining group due to: consumer pro-actively leaving the group
[2024-06-29T17:05:49.511+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO Metrics: Metrics scheduler closed
[2024-06-29T17:05:49.511+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter
[2024-06-29T17:05:49.512+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO Metrics: Metrics reporters closed
[2024-06-29T17:05:49.512+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-fa471307-17b3-410c-876e-fa12c18d7635--1254972573-executor-1 unregistered
[2024-06-29T17:05:49.513+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO SparkContext: Invoking stop() from shutdown hook
[2024-06-29T17:05:49.513+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2024-06-29T17:05:49.520+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO SparkUI: Stopped Spark web UI at http://localhost:4040
[2024-06-29T17:05:49.528+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2024-06-29T17:05:49.535+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO MemoryStore: MemoryStore cleared
[2024-06-29T17:05:49.535+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO BlockManager: BlockManager stopped
[2024-06-29T17:05:49.537+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO BlockManagerMaster: BlockManagerMaster stopped
[2024-06-29T17:05:49.538+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2024-06-29T17:05:49.542+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO SparkContext: Successfully stopped SparkContext
[2024-06-29T17:05:49.543+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO ShutdownHookManager: Shutdown hook called
[2024-06-29T17:05:49.543+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-ea5720a2-b8fe-4ec1-bada-f70ea03e9699
[2024-06-29T17:05:49.544+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709
[2024-06-29T17:05:49.545+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO ShutdownHookManager: Deleting directory /tmp/spark-80187d18-15df-4a21-a95e-6846d4839709/pyspark-7e193b3b-2e01-4872-a085-83a3a86d5d83
[2024-06-29T17:05:49.547+0000] {docker.py:413} INFO - 24/06/29 17:05:49 INFO ShutdownHookManager: Deleting directory /tmp/temporary-b532abb9-8ba6-42e2-8a65-b363e407be65
[2024-06-29T17:05:49.773+0000] {taskinstance.py:1937} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 486, in execute
    return self._run_image()
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 360, in _run_image
    return self._run_image_with_mounts(self.mounts, add_tmp_variable=False)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/docker/operators/docker.py", line 421, in _run_image_with_mounts
    raise DockerContainerFailedException(f"Docker container failed: {result!r}", logs=log_lines)
airflow.providers.docker.exceptions.DockerContainerFailedException: Docker container failed: {'StatusCode': 1}
[2024-06-29T17:05:49.777+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=kafka_spark_dag, task_id=pyspark_consumer, execution_date=20240629T170306, start_date=20240629T170445, end_date=20240629T170549
[2024-06-29T17:05:49.788+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 218 for task pyspark_consumer (Docker container failed: {'StatusCode': 1}; 65234)
[2024-06-29T17:05:49.797+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2024-06-29T17:05:49.810+0000] {taskinstance.py:2778} INFO - 0 downstream tasks scheduled from follow-on schedule check
